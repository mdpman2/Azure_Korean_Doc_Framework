#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
azure_korean_doc_framework v4.0 ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì‹œë‚˜ë¦¬ì˜¤ë³„ í…ŒìŠ¤íŠ¸:
 1. Config v4.0 ì„¤ì • ê²€ì¦ (Graph RAG + êµ¬ì¡°í™” ì¶”ì¶œ ì„¤ì •)
 2. Azure í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ìºì‹±
 3. MultiModelManager (GPT-5.2, max_completion_tokens)
 4. HybridDocumentParser ì´ˆê¸°í™”
 5. AdaptiveChunker ì²­í‚¹ + v4.0 ë©”íƒ€ë°ì´í„° (hangul_ratio, graph_rag_eligible)
 6. KnowledgeGraphManager (LightRAG ê¸°ë°˜) â€” ì˜¤í”„ë¼ì¸ ê·¸ë˜í”„ ì¡°ì‘
 7. KoreanUnicodeTokenizer + CharInterval (í•œê¸€ ìœ„ì¹˜ ë§¤í•‘)
 8. StructuredEntityExtractor ë°ì´í„° ëª¨ë¸ ê²€ì¦
 9. KoreanDocAgent ì´ˆê¸°í™” + Graph-Enhanced êµ¬ì¡°
10. ChunkLogger JSON ì§ë ¬í™”
11. VectorStore ì´ˆê¸°í™”
12. CLI ì¸ì íŒŒì‹± (doc_chunk_main.py v4.0 ì˜µì…˜)
"""

import sys
import os
import json
import tempfile
import shutil

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_ROOT)

# ==================== í…ŒìŠ¤íŠ¸ ìœ í‹¸ë¦¬í‹° ====================

class TestRunner:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìˆ˜ì§‘ ë° ë³´ê³ """
    def __init__(self):
        self.results = []       # [(section, name, status, detail)]  status: pass/fail/skip
        self.current_section = ""

    def section(self, name: str):
        self.current_section = name

    def check(self, name: str, condition: bool, detail: str = ""):
        status = "pass" if condition else "fail"
        icon = "âœ…" if condition else "âŒ"
        self.results.append((self.current_section, name, status, detail))
        print(f"  {icon} {name}" + (f" â€” {detail}" if detail else ""))

    def skip(self, name: str, reason: str = ""):
        self.results.append((self.current_section, name, "skip", reason))
        print(f"  â­ï¸ SKIP: {name}" + (f" â€” {reason}" if reason else ""))

    def summary(self):
        total = len(self.results)
        passed = sum(1 for _, _, s, _ in self.results if s == "pass")
        skipped = sum(1 for _, _, s, _ in self.results if s == "skip")
        failed = total - passed - skipped

        print("\n" + "=" * 70)
        print("ğŸ“Š v4.0 ì¢…í•© í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("=" * 70)

        # ì„¹ì…˜ë³„ ìš”ì•½
        sections = {}
        for sec, name, s, _ in self.results:
            if sec not in sections:
                sections[sec] = {"pass": 0, "fail": 0, "skip": 0}
            sections[sec][s] += 1

        for sec, counts in sections.items():
            total_sec = counts["pass"] + counts["fail"] + counts["skip"]
            if counts["fail"] > 0:
                icon = "âŒ"
            elif counts["skip"] > 0:
                icon = "â­ï¸"
            else:
                icon = "âœ…"
            skip_info = f" ({counts['skip']} skipped)" if counts["skip"] else ""
            print(f"  {icon} {sec}: {counts['pass']}/{total_sec}{skip_info}")

        print(f"\nğŸ ì´ ê²°ê³¼: {passed} í†µê³¼ / {skipped} ìŠ¤í‚µ / {failed} ì‹¤íŒ¨ (ì´ {total}ê°œ)")

        if failed == 0:
            print("\nâœ¨ ëª¨ë“  ì½”ë“œ í…ŒìŠ¤íŠ¸ í†µê³¼! v4.0 ì—…ë°ì´íŠ¸ ê²€ì¦ ì™„ë£Œ")
            if skipped > 0:
                print(f"   ({skipped}ê°œ í™˜ê²½ ë¯¸ì„¤ì •ìœ¼ë¡œ ìŠ¤í‚µ â€” .env ì„¤ì • í›„ ì¬ì‹¤í–‰ ê¶Œì¥)")
        else:
            print("\nâš ï¸ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
            for sec, name, s, detail in self.results:
                if s == "fail":
                    print(f"   âŒ [{sec}] {name}" + (f": {detail}" if detail else ""))

        return 0 if failed == 0 else 1


T = TestRunner()


# ==================== 1. Config v4.0 ì„¤ì • ====================

def test_config_v4():
    T.section("[1] Config v4.0")
    print("\n" + "=" * 70)
    print("ğŸ“‹ [1/12] Config v4.0 ì„¤ì • ê²€ì¦")
    print("=" * 70)

    from azure_korean_doc_framework.config import Config

    # ê¸°ë³¸ ì„¤ì •
    T.check("DEFAULT_MODEL == gpt-5.2", Config.DEFAULT_MODEL == "gpt-5.2", Config.DEFAULT_MODEL)
    T.check("VISION_MODEL == gpt-5.2", Config.VISION_MODEL == "gpt-5.2", Config.VISION_MODEL)
    T.check("PARSING_MODEL == gpt-5.2", Config.PARSING_MODEL == "gpt-5.2", Config.PARSING_MODEL)
    T.check("ADVANCED_MODELS is frozenset", isinstance(Config.ADVANCED_MODELS, frozenset))
    T.check("REASONING_MODELS is frozenset", isinstance(Config.REASONING_MODELS, frozenset))
    T.check("STRUCTURED_OUTPUT_MODELS is frozenset", isinstance(Config.STRUCTURED_OUTPUT_MODELS, frozenset))
    T.check("gpt-5.2 in ADVANCED_MODELS", "gpt-5.2" in Config.ADVANCED_MODELS)
    T.check("gpt-5.2 in REASONING_MODELS", "gpt-5.2" in Config.REASONING_MODELS)

    # v4.0 Graph RAG ì„¤ì •
    T.check("GRAPH_RAG_ENABLED is bool", isinstance(Config.GRAPH_RAG_ENABLED, bool), str(Config.GRAPH_RAG_ENABLED))
    T.check("GRAPH_STORAGE_PATH set", bool(Config.GRAPH_STORAGE_PATH), Config.GRAPH_STORAGE_PATH)
    T.check("GRAPH_ENTITY_BATCH_SIZE is int", isinstance(Config.GRAPH_ENTITY_BATCH_SIZE, int), str(Config.GRAPH_ENTITY_BATCH_SIZE))
    T.check("GRAPH_QUERY_MODE valid", Config.GRAPH_QUERY_MODE in ("local", "global", "hybrid", "naive"), Config.GRAPH_QUERY_MODE)
    T.check("GRAPH_TOP_K is int > 0", isinstance(Config.GRAPH_TOP_K, int) and Config.GRAPH_TOP_K > 0, str(Config.GRAPH_TOP_K))

    # v4.0 êµ¬ì¡°í™” ì¶”ì¶œ ì„¤ì •
    T.check("EXTRACTION_PASSES is int >= 1", isinstance(Config.EXTRACTION_PASSES, int) and Config.EXTRACTION_PASSES >= 1, str(Config.EXTRACTION_PASSES))
    T.check("EXTRACTION_MAX_CHUNK_CHARS is int", isinstance(Config.EXTRACTION_MAX_CHUNK_CHARS, int), str(Config.EXTRACTION_MAX_CHUNK_CHARS))
    T.check("EXTRACTION_MAX_WORKERS is int", isinstance(Config.EXTRACTION_MAX_WORKERS, int), str(Config.EXTRACTION_MAX_WORKERS))

    # ëª¨ë¸ ë§¤í•‘
    T.check("MODELS has gpt-5.2", "gpt-5.2" in Config.MODELS)
    T.check("gpt-5.2 â†’ model-router", Config.MODELS.get("gpt-5.2") == "model-router", Config.MODELS.get("gpt-5.2", ""))

    # EMBEDDING ì„¤ì •
    T.check("EMBEDDING_DEPLOYMENT set", bool(Config.EMBEDDING_DEPLOYMENT), Config.EMBEDDING_DEPLOYMENT)
    T.check("EMBEDDING_DIMENSIONS is int", isinstance(Config.EMBEDDING_DIMENSIONS, int), str(Config.EMBEDDING_DIMENSIONS))


# ==================== 2. Azure í´ë¼ì´ì–¸íŠ¸ ====================

def test_azure_clients():
    T.section("[2] Azure Clients")
    print("\n" + "=" * 70)
    print("ğŸ”Œ [2/12] Azure í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë° ìºì‹±")
    print("=" * 70)

    from azure_korean_doc_framework.utils.azure_clients import AzureClientFactory
    from azure_korean_doc_framework.config import Config

    # OpenAI í´ë¼ì´ì–¸íŠ¸
    if not Config.OPENAI_API_KEY:
        T.skip("Standard OpenAI Client", "AZURE_OPENAI_API_KEY ë¯¸ì„¤ì •")
        T.skip("Advanced OpenAI Client", "AZURE_OPENAI_API_KEY ë¯¸ì„¤ì •")
        T.skip("Client caching", "AZURE_OPENAI_API_KEY ë¯¸ì„¤ì •")
    else:
        try:
            client_std = AzureClientFactory.get_openai_client(is_advanced=False)
            T.check("Standard OpenAI Client", client_std is not None)
        except Exception as e:
            T.check("Standard OpenAI Client", False, str(e))

        try:
            client_adv = AzureClientFactory.get_openai_client(is_advanced=True)
            T.check("Advanced OpenAI Client", client_adv is not None)
        except Exception as e:
            T.check("Advanced OpenAI Client", False, str(e))

        try:
            client_adv2 = AzureClientFactory.get_openai_client(is_advanced=True)
            T.check("Client caching (same instance)", client_adv is client_adv2)
        except Exception as e:
            T.check("Client caching", False, str(e))

    # Document Intelligence í´ë¼ì´ì–¸íŠ¸
    if not Config.DI_KEY:
        T.skip("DI Client", "AZURE_DI_KEY ë¯¸ì„¤ì •")
    else:
        try:
            di_client = AzureClientFactory.get_di_client()
            T.check("DI Client", di_client is not None)
        except Exception as e:
            T.check("DI Client", False, str(e))


# ==================== 3. MultiModelManager ====================

def test_multi_model_manager():
    T.section("[3] MultiModelManager")
    print("\n" + "=" * 70)
    print("ğŸ¤– [3/12] MultiModelManager GPT-5.2 í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    from azure_korean_doc_framework.core.multi_model_manager import MultiModelManager
    from azure_korean_doc_framework.config import Config

    manager = MultiModelManager()
    T.check("default_model == gpt-5.2", manager.default_model == "gpt-5.2")

    # ì»¤ìŠ¤í…€ ëª¨ë¸ë¡œ ì´ˆê¸°í™”
    custom_mgr = MultiModelManager(default_model="gpt-4.1")
    T.check("custom default_model", custom_mgr.default_model == "gpt-4.1")

    # API í˜¸ì¶œ í…ŒìŠ¤íŠ¸ (ì—”ë“œí¬ì¸íŠ¸ + ë°°í¬ ì¡´ì¬ ì‹œì—ë§Œ)
    from azure_korean_doc_framework.config import Config
    if not Config.OPENAI_API_KEY_5 and not Config.OPENAI_API_KEY:
        T.skip("GPT-5.2 API call", "Azure OpenAI í‚¤ ë¯¸ì„¤ì •")
    else:
        print("  ğŸ”„ GPT-5.2 API í˜¸ì¶œ ì¤‘...")
        try:
            response = manager.get_completion(
                prompt="'í…ŒìŠ¤íŠ¸ ì„±ê³µ'ì´ë¼ê³ ë§Œ ë‹µí•´ì£¼ì„¸ìš”.",
                model_key="gpt-5.2",
                temperature=0.0,
                max_tokens=50
            )
            success = response and not response.startswith("âŒ")
            if not success and "DeploymentNotFound" in (response or ""):
                T.skip("GPT-5.2 API call", "model-router ë°°í¬ ë¯¸ì¡´ì¬ (Azure Portalì—ì„œ ìƒì„± í•„ìš”)")
            else:
                T.check("GPT-5.2 API call", success, response[:80] if response else "(empty)")
        except Exception as e:
            if "DeploymentNotFound" in str(e):
                T.skip("GPT-5.2 API call", "model-router ë°°í¬ ë¯¸ì¡´ì¬")
            else:
                T.check("GPT-5.2 API call", False, str(e))


# ==================== 4. Parser ì´ˆê¸°í™” ====================

def test_parser():
    T.section("[4] Parser")
    print("\n" + "=" * 70)
    print("ğŸ“„ [4/12] HybridDocumentParser ì´ˆê¸°í™”")
    print("=" * 70)

    from azure_korean_doc_framework.config import Config
    if not Config.DI_KEY:
        T.skip("Parser ì´ˆê¸°í™”", "AZURE_DI_KEY ë¯¸ì„¤ì • (Document Intelligence í•„ìš”)")
        T.skip("has gpt_model attr", "AZURE_DI_KEY ë¯¸ì„¤ì •")
        return

    from azure_korean_doc_framework.parsing.parser import HybridDocumentParser

    try:
        parser = HybridDocumentParser()
        T.check("Parser ì´ˆê¸°í™”", True)
        has_model = hasattr(parser, 'gpt_model')
        T.check("has gpt_model attr", has_model, getattr(parser, 'gpt_model', 'N/A'))
    except Exception as e:
        T.check("Parser ì´ˆê¸°í™”", False, str(e))


# ==================== 5. AdaptiveChunker + v4.0 ë©”íƒ€ë°ì´í„° ====================

def test_chunker_v4():
    T.section("[5] Chunker v4.0")
    print("\n" + "=" * 70)
    print("âœ‚ï¸ [5/12] AdaptiveChunker + v4.0 ë©”íƒ€ë°ì´í„°")
    print("=" * 70)

    from azure_korean_doc_framework.parsing.chunker import AdaptiveChunker, ChunkingConfig, ChunkingStrategy

    # ê¸°ë³¸ ì´ˆê¸°í™”
    chunker = AdaptiveChunker()
    T.check("AdaptiveChunker ì´ˆê¸°í™”", chunker is not None)
    T.check("encoder loaded", chunker.encoder is not None)

    # í† í° ì¹´ìš´íŠ¸
    token_count = chunker._count_tokens("ì•ˆë…•í•˜ì„¸ìš” í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.")
    T.check("_count_tokens > 0", token_count > 0, f"tokens={token_count}")

    # í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬
    sentences = chunker._split_korean_sentences("ì²« ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤. ì„¸ ë²ˆì§¸.")
    T.check("_split_korean_sentences >= 2", len(sentences) >= 2, f"sentences={len(sentences)}")

    # v4.0: í•œê¸€ ë¹„ìœ¨ ê³„ì‚°
    ratio_kr = chunker._calculate_hangul_ratio("í•œêµ­ì–´ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤")
    ratio_en = chunker._calculate_hangul_ratio("This is English text only")
    ratio_empty = chunker._calculate_hangul_ratio("")
    T.check("hangul_ratio (í•œêµ­ì–´) > 0.3", ratio_kr > 0.3, f"ratio={ratio_kr}")
    T.check("hangul_ratio (ì˜ì–´) == 0.0", ratio_en == 0.0, f"ratio={ratio_en}")
    T.check("hangul_ratio (ë¹ˆë¬¸ì) == 0.0", ratio_empty == 0.0)

    # ì²­í‚¹ í…ŒìŠ¤íŠ¸ (hierarchical segments)
    test_segments = [
        {"type": "header", "content": "# 1ì¥ ì„œë¡ ", "page": 1},
        {"type": "text", "content": "í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ì„ ìœ„í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. " * 20, "page": 1},
        {"type": "table", "content": "| í•­ëª© | ê°’ |\n|---|---|\n| A | 100 |", "page": 2},
        {"type": "image", "content": "> **[ì´ë¯¸ì§€/ì°¨íŠ¸ ì„¤ëª…]** ê·¸ë˜í”„ ì´ë¯¸ì§€", "page": 2},
        {"type": "text", "content": "ë‘ ë²ˆì§¸ í…ìŠ¤íŠ¸ ì„¹ì…˜ì…ë‹ˆë‹¤. ì¶”ê°€ì ì¸ ë‚´ìš©ì´ ì—¬ê¸°ì— ë“¤ì–´ê°‘ë‹ˆë‹¤. " * 15, "page": 3},
    ]

    chunks = chunker.chunk(test_segments, filename="test_doc.pdf", extra_metadata={"source": "test"})
    T.check("chunk() returns list", isinstance(chunks, list) and len(chunks) > 0, f"chunks={len(chunks)}")

    # v4.0 ë©”íƒ€ë°ì´í„° ê²€ì¦
    has_hangul_ratio = any("hangul_ratio" in c.metadata for c in chunks)
    T.check("hangul_ratio in metadata", has_hangul_ratio)

    has_graph_eligible = any("graph_rag_eligible" in c.metadata for c in chunks)
    T.check("graph_rag_eligible in metadata", has_graph_eligible)

    # graph_rag_eligibleì€ í…ìŠ¤íŠ¸ ì²­í¬ì—ë§Œ True
    text_chunks = [c for c in chunks if c.metadata.get("graph_rag_eligible")]
    table_chunks = [c for c in chunks if c.metadata.get("is_table_data")]
    image_chunks = [c for c in chunks if c.metadata.get("is_image_data")]
    T.check("text chunks have graph_rag_eligible", len(text_chunks) > 0)
    T.check("table chunks exist", len(table_chunks) > 0, f"count={len(table_chunks)}")

    # table/image ì²­í¬ì—ëŠ” graph_rag_eligible ì—†ì–´ì•¼ í•¨
    bad_table = [c for c in table_chunks if c.metadata.get("graph_rag_eligible")]
    bad_image = [c for c in image_chunks if c.metadata.get("graph_rag_eligible")]
    T.check("table chunks NOT graph_rag_eligible", len(bad_table) == 0)
    T.check("image chunks NOT graph_rag_eligible", len(bad_image) == 0)

    # ë©”íƒ€ë°ì´í„° í•„ìˆ˜ í•„ë“œ í™•ì¸
    sample = chunks[0].metadata
    for field in ["chunk_index", "total_chunks", "token_count", "char_count"]:
        T.check(f"metadata has '{field}'", field in sample, str(sample.get(field, "MISSING")))

    # ë¬¸ì„œ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸
    strategy = chunker._classify_document("test_report.pdf", test_segments)
    T.check("_classify_document returns ChunkingStrategy", isinstance(strategy, ChunkingStrategy), strategy.name)


# ==================== 6. KnowledgeGraphManager ====================

def test_graph_rag():
    T.section("[6] Graph RAG")
    print("\n" + "=" * 70)
    print("ğŸ“Š [6/12] KnowledgeGraphManager (LightRAG ê¸°ë°˜)")
    print("=" * 70)

    try:
        from azure_korean_doc_framework.core.graph_rag import (
            KnowledgeGraphManager, Entity, Relationship,
            GraphQueryResult, QueryMode, KOREAN_ENTITY_TYPES,
        )
        import networkx as nx
    except ImportError as e:
        T.check("networkx import", False, str(e))
        return

    # ë°ì´í„° ëª¨ë¸ ê²€ì¦
    T.check("QueryMode has LOCAL", hasattr(QueryMode, "LOCAL"))
    T.check("QueryMode has GLOBAL", hasattr(QueryMode, "GLOBAL"))
    T.check("QueryMode has HYBRID", hasattr(QueryMode, "HYBRID"))
    T.check("QueryMode has NAIVE", hasattr(QueryMode, "NAIVE"))
    T.check("KOREAN_ENTITY_TYPES >= 14", len(KOREAN_ENTITY_TYPES) >= 14, f"count={len(KOREAN_ENTITY_TYPES)}")

    # Entity ë°ì´í„° ëª¨ë¸
    entity = Entity(name="ì‚¼ì„±ì „ì", entity_type="ì¡°ì§", description="í•œêµ­ ëŒ€ê¸°ì—…")
    T.check("Entity.name", entity.name == "ì‚¼ì„±ì „ì")
    T.check("Entity.entity_id (md5)", len(entity.entity_id) == 12)
    T.check("Entity.source_chunks default", entity.source_chunks == [])

    # Relationship ë°ì´í„° ëª¨ë¸
    rel = Relationship(source="ì´ì¬ìš©", target="ì‚¼ì„±ì „ì", relation_type="ì†Œì†", description="íšŒì¥")
    T.check("Relationship fields", rel.source == "ì´ì¬ìš©" and rel.target == "ì‚¼ì„±ì „ì")
    T.check("Relationship.weight default", rel.weight == 1.0)

    # GraphQueryResult
    result = GraphQueryResult()
    T.check("GraphQueryResult defaults", result.entities == [] and result.relationships == [])

    # KnowledgeGraphManager ì´ˆê¸°í™” (LLM í˜¸ì¶œ ì—†ì´ ê·¸ë˜í”„ ì¡°ì‘ í…ŒìŠ¤íŠ¸)
    gm = KnowledgeGraphManager()
    T.check("KnowledgeGraphManager init", gm is not None)
    T.check("graph is DiGraph", isinstance(gm.graph, nx.DiGraph))
    T.check("entity_types loaded", len(gm.entity_types) == len(KOREAN_ENTITY_TYPES))
    T.check("_is_gpt5 cached", gm._is_gpt5 is True)

    # ì—”í‹°í‹° ì¶”ê°€
    gm._add_entity(Entity(name="ì‚¼ì„±ì „ì", entity_type="ì¡°ì§", description="ë°˜ë„ì²´ ê¸°ì—…", source_chunks=["c1"]))
    gm._add_entity(Entity(name="ì´ì¬ìš©", entity_type="ì¸ë¬¼", description="ì‚¼ì„±ì „ì íšŒì¥", source_chunks=["c1"]))
    T.check("graph nodes after add", gm.graph.number_of_nodes() == 2, f"nodes={gm.graph.number_of_nodes()}")

    # ì—”í‹°í‹° ì¤‘ë³µ ë³‘í•© (ë” ê¸´ descriptionìœ¼ë¡œ ì—…ë°ì´íŠ¸)
    gm._add_entity(Entity(name="ì‚¼ì„±ì „ì", entity_type="ì¡°ì§", description="ì„¸ê³„ ìµœëŒ€ ë°˜ë„ì²´ ì œì¡° ê¸°ì—…", source_chunks=["c2"]))
    T.check("entity merge (still 2 nodes)", gm.graph.number_of_nodes() == 2)
    T.check("entity desc updated (longer)", gm.graph.nodes["ì‚¼ì„±ì „ì"]["description"] == "ì„¸ê³„ ìµœëŒ€ ë°˜ë„ì²´ ì œì¡° ê¸°ì—…")

    # ê´€ê³„ ì¶”ê°€
    gm._add_relationship(Relationship(source="ì´ì¬ìš©", target="ì‚¼ì„±ì „ì", relation_type="ì†Œì†", description="íšŒì¥", weight=1.5, keywords="ê²½ì˜,ë¦¬ë”ì‹­"))
    T.check("graph edges after add", gm.graph.number_of_edges() == 1)

    # ê´€ê³„ ì¤‘ë³µ â†’ ê°€ì¤‘ì¹˜ í•©ì‚°
    gm._add_relationship(Relationship(source="ì´ì¬ìš©", target="ì‚¼ì„±ì „ì", relation_type="ì†Œì†", description="ëŒ€í‘œ", weight=0.5))
    T.check("edge weight accumulated", gm.graph["ì´ì¬ìš©"]["ì‚¼ì„±ì „ì"]["weight"] == 2.0)

    # ì—†ëŠ” ë…¸ë“œ ê°„ ê´€ê³„ â†’ ìë™ ë…¸ë“œ ìƒì„±
    gm._add_relationship(Relationship(source="SKí•˜ì´ë‹‰ìŠ¤", target="ë°˜ë„ì²´ ì‹œì¥", relation_type="ì°¸ì—¬", description="ê²½ìŸ"))
    T.check("auto-create nodes for edge", gm.graph.has_node("SKí•˜ì´ë‹‰ìŠ¤") and gm.graph.has_node("ë°˜ë„ì²´ ì‹œì¥"))

    # get_stats
    stats = gm.get_stats()
    T.check("get_stats().nodes > 0", stats["nodes"] > 0, f"nodes={stats['nodes']}, edges={stats['edges']}")
    T.check("get_stats().entity_types", len(stats["entity_types"]) > 0)
    T.check("get_stats().avg_degree", stats["avg_degree"] > 0)

    # get_subgraph (BFS)
    subgraph = gm.get_subgraph("ì´ì¬ìš©", max_depth=2, max_nodes=10)
    T.check("get_subgraph has nodes", len(subgraph["nodes"]) > 0)
    T.check("get_subgraph has edges", len(subgraph["edges"]) > 0)
    T.check("subgraph(ì—†ëŠ”ì—”í‹°í‹°) empty", gm.get_subgraph("ì—†ëŠ”ì—”í‹°í‹°") == {"nodes": [], "edges": []})

    # _build_context_text
    qr = GraphQueryResult(
        entities=[Entity(name="ì‚¼ì„±ì „ì", entity_type="ì¡°ì§", description="ë°˜ë„ì²´ ê¸°ì—…")],
        relationships=[Relationship(source="ì´ì¬ìš©", target="ì‚¼ì„±ì „ì", relation_type="ì†Œì†", description="íšŒì¥")],
    )
    ctx = gm._build_context_text(qr)
    T.check("_build_context_text non-empty", len(ctx) > 0)
    T.check("context contains entity", "ì‚¼ì„±ì „ì" in ctx)
    T.check("context contains relation", "ì´ì¬ìš©" in ctx and "ì†Œì†" in ctx)

    # save/load (ì„ì‹œ íŒŒì¼)
    with tempfile.NamedTemporaryFile(suffix=".json", delete=False, mode="w") as tmp:
        tmp_path = tmp.name

    try:
        gm.save_graph(tmp_path)
        T.check("save_graph succeeds", os.path.exists(tmp_path))

        # ìƒˆ ë§¤ë‹ˆì €ì—ì„œ ë¡œë“œ
        gm2 = KnowledgeGraphManager()
        gm2.load_graph(tmp_path)
        T.check("load_graph nodes match", gm2.graph.number_of_nodes() == gm.graph.number_of_nodes())
        T.check("load_graph edges match", gm2.graph.number_of_edges() == gm.graph.number_of_edges())

        # JSON êµ¬ì¡° ê²€ì¦
        with open(tmp_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        T.check("JSON has 'nodes' key", "nodes" in data)
        T.check("JSON has 'edges' key", "edges" in data)
    finally:
        os.unlink(tmp_path)

    # clear
    gm.clear()
    T.check("clear() empties graph", gm.graph.number_of_nodes() == 0 and gm.graph.number_of_edges() == 0)


# ==================== 7. KoreanUnicodeTokenizer ====================

def test_korean_tokenizer():
    T.section("[7] KoreanUnicodeTokenizer")
    print("\n" + "=" * 70)
    print("ğŸ”¤ [7/12] KoreanUnicodeTokenizer + CharInterval")
    print("=" * 70)

    from azure_korean_doc_framework.parsing.entity_extractor import (
        KoreanUnicodeTokenizer, CharInterval, _map_normalized_to_original,
    )

    tok = KoreanUnicodeTokenizer()

    # CharInterval
    ci = CharInterval(start_pos=0, end_pos=5)
    T.check("CharInterval.length", ci.length == 5)

    # is_hangul
    T.check("is_hangul('ê°€')", tok.is_hangul("ê°€"))
    T.check("is_hangul('A') == False", not tok.is_hangul("A"))
    T.check("is_hangul('æ¼¢') == False", not tok.is_hangul("æ¼¢"))

    # count_hangul_ratio
    ratio = tok.count_hangul_ratio("ì•ˆë…•í•˜ì„¸ìš” hello")
    T.check("count_hangul_ratio mixed > 0", ratio > 0 and ratio < 1, f"ratio={ratio:.3f}")
    T.check("count_hangul_ratio pure korean", tok.count_hangul_ratio("ëŒ€í•œë¯¼êµ­") == 1.0)
    T.check("count_hangul_ratio pure english", tok.count_hangul_ratio("hello") == 0.0)
    T.check("count_hangul_ratio empty", tok.count_hangul_ratio("") == 0.0)

    # find_text_positions â€” ì •í™•í•œ ë§¤ì¹­
    text = "ì‚¼ì„±ì „ìëŠ” í•œêµ­ì˜ ëŒ€í‘œì ì¸ ê¸°ì—…ì´ë‹¤. ì‚¼ì„±ì „ìëŠ” ë°˜ë„ì²´ë¥¼ ì œì¡°í•œë‹¤."
    positions = tok.find_text_positions(text, "ì‚¼ì„±ì „ì")
    T.check("find_text_positions >= 2 matches", len(positions) >= 2, f"matches={len(positions)}")
    if positions:
        T.check("first match start_pos == 0", positions[0].start_pos == 0)
        T.check("first match end_pos == 4", positions[0].end_pos == 4)

    # find_text_positions â€” ì—†ëŠ” í…ìŠ¤íŠ¸
    no_match = tok.find_text_positions(text, "ì¡´ì¬í•˜ì§€ì•ŠëŠ”í…ìŠ¤íŠ¸")
    T.check("no match returns []", len(no_match) == 0)

    # find_text_positions â€” í¼ì§€ ë§¤ì¹­ (ê³µë°± ì°¨ì´)
    text_with_spaces = "ì‚¼ ì„±  ì „ìëŠ” ì¢‹ì€ ê¸°ì—…ì´ë‹¤"
    fuzzy = tok.find_text_positions(text_with_spaces, "ì‚¼ ì„± ì „ì", fuzzy=True)
    T.check("fuzzy matching", len(fuzzy) >= 0)  # fuzzyëŠ” êµ¬í˜„ì— ë”°ë¼ ê²°ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ

    # _map_normalized_to_original
    original = "Hello   World"
    mapped = _map_normalized_to_original(original, 6)  # "Hello " ì´í›„
    T.check("_map_normalized_to_original", mapped >= 5, f"mapped={mapped}")


# ==================== 8. StructuredEntityExtractor ====================

def test_entity_extractor_models():
    T.section("[8] EntityExtractor ëª¨ë¸")
    print("\n" + "=" * 70)
    print("ğŸ“‹ [8/12] StructuredEntityExtractor ë°ì´í„° ëª¨ë¸")
    print("=" * 70)

    from azure_korean_doc_framework.parsing.entity_extractor import (
        Extraction, ExampleData, ExtractionResult, CharInterval,
        DEFAULT_KOREAN_EXAMPLES, StructuredEntityExtractor,
    )

    # Extraction ë°ì´í„° ëª¨ë¸
    ext = Extraction(
        extraction_class="ì¸ë¬¼",
        extraction_text="ì´ì¬ìš©",
        char_interval=CharInterval(0, 3),
        attributes={"ì§í•¨": "íšŒì¥"},
        description="ì‚¼ì„±ì „ì íšŒì¥",
    )
    T.check("Extraction fields", ext.extraction_class == "ì¸ë¬¼" and ext.extraction_text == "ì´ì¬ìš©")
    T.check("Extraction.char_interval", ext.char_interval.length == 3)
    T.check("Extraction.alignment_status default", ext.alignment_status == "aligned")

    # ExampleData
    T.check("DEFAULT_KOREAN_EXAMPLES non-empty", len(DEFAULT_KOREAN_EXAMPLES) > 0)
    example = DEFAULT_KOREAN_EXAMPLES[0]
    T.check("Example has text", bool(example.text))
    T.check("Example has extractions", len(example.extractions) > 0)

    # ExtractionResult
    result = ExtractionResult(text="í…ŒìŠ¤íŠ¸", extractions=[ext], processing_time=1.5, num_chunks=1, num_passes=1)
    T.check("ExtractionResult fields", result.num_chunks == 1 and result.processing_time == 1.5)

    # StructuredEntityExtractor ì´ˆê¸°í™” (LLM í˜¸ì¶œ ì—†ì´)
    extractor = StructuredEntityExtractor()
    T.check("Extractor ì´ˆê¸°í™”", extractor is not None)
    T.check("Extractor._is_gpt5 cached", extractor._is_gpt5 is True)
    T.check("Extractor._system_prompt cached", bool(extractor._system_prompt))
    T.check("Extractor.tokenizer", extractor.tokenizer is not None)

    # _chunk_text í…ŒìŠ¤íŠ¸
    short_text = "ì§§ì€ í…ìŠ¤íŠ¸"
    chunks = extractor._chunk_text(short_text)
    T.check("_chunk_text (short) returns 1", len(chunks) == 1)

    long_text = "í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤. " * 500  # ~5000+ chars
    chunks_long = extractor._chunk_text(long_text)
    T.check("_chunk_text (long) splits", len(chunks_long) > 1, f"chunks={len(chunks_long)}")

    # _format_examples
    examples_text = extractor._format_examples()
    T.check("_format_examples non-empty", len(examples_text) > 0)
    T.check("_format_examples contains class", "ì¡°ì§" in examples_text or "ì¸ë¬¼" in examples_text)

    # _deduplicate
    dups = [
        Extraction(extraction_class="ì¸ë¬¼", extraction_text="ì´ì¬ìš©"),
        Extraction(extraction_class="ì¸ë¬¼", extraction_text="ì´ì¬ìš©"),
        Extraction(extraction_class="ì¡°ì§", extraction_text="ì‚¼ì„±ì „ì"),
    ]
    unique = extractor._deduplicate(dups)
    T.check("_deduplicate removes dups", len(unique) == 2, f"unique={len(unique)}")

    # _merge_extractions (single pass)
    merged = extractor._merge_extractions([dups])
    T.check("_merge_extractions single pass", len(merged) == 2)

    # _merge_extractions (multi pass)
    pass1 = [Extraction(extraction_class="ì¸ë¬¼", extraction_text="ì´ì¬ìš©")]
    pass2 = [Extraction(extraction_class="ì¸ë¬¼", extraction_text="ì´ì¬ìš©"),
             Extraction(extraction_class="ì¡°ì§", extraction_text="LGì „ì")]
    merged_multi = extractor._merge_extractions([pass1, pass2])
    T.check("_merge_extractions multi pass", len(merged_multi) == 2)

    # _ground_extractions
    text = "ì‚¼ì„±ì „ì ì´ì¬ìš© íšŒì¥ì´ ë°œí‘œí–ˆë‹¤."
    exts = [
        Extraction(extraction_class="ì¡°ì§", extraction_text="ì‚¼ì„±ì „ì"),
        Extraction(extraction_class="ì¸ë¬¼", extraction_text="ì´ì¬ìš©"),
        Extraction(extraction_class="ê¸°íƒ€", extraction_text="ì¡´ì¬í•˜ì§€ì•ŠëŠ”í…ìŠ¤íŠ¸"),
    ]
    extractor._ground_extractions(text, exts)
    T.check("grounding: ì‚¼ì„±ì „ì aligned", exts[0].alignment_status == "aligned" and exts[0].char_interval is not None)
    T.check("grounding: ì´ì¬ìš© aligned", exts[1].alignment_status == "aligned")
    T.check("grounding: ë¯¸ì¡´ì¬ unaligned", exts[2].alignment_status == "unaligned")

    # extractions_to_dict
    result_for_dict = ExtractionResult(text=text, extractions=exts[:2])
    dicts = extractor.extractions_to_dict(result_for_dict)
    T.check("extractions_to_dict returns list", isinstance(dicts, list) and len(dicts) == 2)
    T.check("dict has extraction_class", "extraction_class" in dicts[0])
    T.check("dict has char_interval", "char_interval" in dicts[0])
    T.check("dict char_interval has start/end", dicts[0]["char_interval"]["start"] == 0 if dicts[0]["char_interval"] else True)


# ==================== 9. KoreanDocAgent ====================

def test_agent_v4():
    T.section("[9] Agent v4.0")
    print("\n" + "=" * 70)
    print("ğŸ” [9/12] KoreanDocAgent v4.0 êµ¬ì¡° ê²€ì¦")
    print("=" * 70)

    from azure_korean_doc_framework.config import Config
    if not Config.SEARCH_KEY:
        T.skip("Agent ì´ˆê¸°í™”", "AZURE_SEARCH_KEY ë¯¸ì„¤ì •")
        T.skip("Agent ì†ì„± ê²€ì¦", "AZURE_SEARCH_KEY ë¯¸ì„¤ì •")
        T.skip("graph_manager ì£¼ì…", "AZURE_SEARCH_KEY ë¯¸ì„¤ì •")
        T.skip("ë©”ì„œë“œ ì¡´ì¬ ê²€ì¦", "AZURE_SEARCH_KEY ë¯¸ì„¤ì •")

        # ëŒ€ì‹  ëª¨ë“ˆ + í´ë˜ìŠ¤ import ê²€ì¦
        from azure_korean_doc_framework.core.agent import KoreanDocAgent
        import inspect
        sig = inspect.signature(KoreanDocAgent.__init__)
        T.check("KoreanDocAgent class importable", True)
        T.check("__init__ has graph_manager param", "graph_manager" in sig.parameters)
        T.check("has graph_enhanced_answer method", hasattr(KoreanDocAgent, 'graph_enhanced_answer'))
        T.check("has _vector_search method", hasattr(KoreanDocAgent, '_vector_search'))
        T.check("has answer_question method", hasattr(KoreanDocAgent, 'answer_question'))
        return

    from azure_korean_doc_framework.core.agent import KoreanDocAgent

    # ê¸°ë³¸ ì´ˆê¸°í™”
    agent = KoreanDocAgent()
    T.check("Agent ì´ˆê¸°í™”", agent is not None)
    T.check("embedding_client", agent.embedding_client is not None)
    T.check("llm_client", agent.llm_client is not None)
    T.check("search_client", agent.search_client is not None)
    T.check("model_manager", agent.model_manager is not None)
    T.check("enable_query_rewrite", agent.enable_query_rewrite is True)
    T.check("graph_manager default None", agent.graph_manager is None)

    # v4.0: graph_manager ì£¼ì…
    try:
        from azure_korean_doc_framework.core.graph_rag import KnowledgeGraphManager
        gm = KnowledgeGraphManager()
        agent_with_graph = KoreanDocAgent(graph_manager=gm)
        T.check("Agent with graph_manager", agent_with_graph.graph_manager is gm)
    except Exception as e:
        T.check("Agent with graph_manager", False, str(e))

    T.check("has _vector_search method", hasattr(agent, '_vector_search') and callable(agent._vector_search))
    T.check("has graph_enhanced_answer", hasattr(agent, 'graph_enhanced_answer') and callable(agent.graph_enhanced_answer))
    T.check("has answer_question", hasattr(agent, 'answer_question') and callable(agent.answer_question))


# ==================== 10. ChunkLogger ====================

def test_chunk_logger():
    T.section("[10] ChunkLogger")
    print("\n" + "=" * 70)
    print("ğŸ“ [10/12] ChunkLogger JSON ì§ë ¬í™”")
    print("=" * 70)

    from azure_korean_doc_framework.utils.logger import ChunkLogger
    from azure_korean_doc_framework.core.schema import Document

    # Document ë°ì´í„° ëª¨ë¸
    doc = Document(page_content="í…ŒìŠ¤íŠ¸ ì½˜í…ì¸ ", metadata={"source": "test.pdf", "hangul_ratio": 0.8})
    T.check("Document.page_content", doc.page_content == "í…ŒìŠ¤íŠ¸ ì½˜í…ì¸ ")
    T.check("Document.metadata", doc.metadata["source"] == "test.pdf")

    # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥
    tmpdir = tempfile.mkdtemp()
    try:
        chunks = [
            Document(page_content="ì²« ë²ˆì§¸ ì²­í¬", metadata={"chunk_index": 0, "hangul_ratio": 0.9, "graph_rag_eligible": True}),
            Document(page_content="ë‘ ë²ˆì§¸ ì²­í¬", metadata={"chunk_index": 1, "is_table_data": True}),
        ]

        result_path = ChunkLogger.save_chunks_to_json(chunks, "test_doc.pdf", output_dir=tmpdir)
        T.check("save_chunks_to_json returns path", result_path is not None)
        T.check("output file exists", os.path.exists(result_path))

        # JSON êµ¬ì¡° ê²€ì¦
        with open(result_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        T.check("JSON is list", isinstance(data, list))
        T.check("JSON length == 2", len(data) == 2)
        T.check("JSON[0] has page_content", "page_content" in data[0])
        T.check("JSON[0] has metadata", "metadata" in data[0])
        T.check("v4.0 hangul_ratio in JSON", data[0]["metadata"].get("hangul_ratio") == 0.9)
        T.check("v4.0 graph_rag_eligible in JSON", data[0]["metadata"].get("graph_rag_eligible") is True)
    finally:
        shutil.rmtree(tmpdir)


# ==================== 11. VectorStore ====================

def test_vector_store():
    T.section("[11] VectorStore")
    print("\n" + "=" * 70)
    print("ğŸ“¦ [11/12] VectorStore ì´ˆê¸°í™”")
    print("=" * 70)

    from azure_korean_doc_framework.config import Config

    if not Config.SEARCH_KEY:
        T.skip("VectorStore ì´ˆê¸°í™”", "AZURE_SEARCH_KEY ë¯¸ì„¤ì •")
        # í´ë˜ìŠ¤ import ê²€ì¦ë§Œ ìˆ˜í–‰
        from azure_korean_doc_framework.core.vector_store import VectorStore
        import inspect
        sig = inspect.signature(VectorStore.__init__)
        T.check("VectorStore class importable", True)
        T.check("VectorStore has upload_documents", hasattr(VectorStore, 'upload_documents'))
        T.check("VectorStore has create_index_if_not_exists", hasattr(VectorStore, 'create_index_if_not_exists'))
        return

    from azure_korean_doc_framework.core.vector_store import VectorStore

    try:
        vs = VectorStore()
        T.check("VectorStore ì´ˆê¸°í™”", vs is not None)
        T.check("index_name set", bool(vs.index_name), vs.index_name)
        T.check("openai_client", vs.openai_client is not None)
    except Exception as e:
        T.check("VectorStore ì´ˆê¸°í™”", False, str(e))


# ==================== 12. CLI ì¸ì íŒŒì‹± ====================

def test_cli_args():
    T.section("[12] CLI v4.0 Args")
    print("\n" + "=" * 70)
    print("âŒ¨ï¸ [12/12] CLI ì¸ì íŒŒì‹± (v4.0 ì˜µì…˜)")
    print("=" * 70)

    # doc_chunk_main.pyì˜ argparseë¥¼ ì‹œë®¬ë ˆì´ì…˜
    import argparse

    arg_parser = argparse.ArgumentParser()
    arg_parser.add_argument("-p", "--path", action="append", default=[])
    arg_parser.add_argument("-q", "--question", type=str, default="ê¸°ë³¸ ì§ˆë¬¸")
    arg_parser.add_argument("--skip-qa", action="store_true")
    arg_parser.add_argument("--skip-ingest", action="store_true")
    arg_parser.add_argument("-w", "--workers", type=int, default=3)
    arg_parser.add_argument("-m", "--model", type=str, default="gpt-5.2")
    # v4.0 ì˜µì…˜
    arg_parser.add_argument("--graph-rag", action="store_true")
    arg_parser.add_argument("--graph-mode", type=str, default="hybrid", choices=["local", "global", "hybrid", "naive"])
    arg_parser.add_argument("--extract-entities", action="store_true")
    arg_parser.add_argument("--graph-save", type=str, default="output/knowledge_graph.json")

    # ì‹œë‚˜ë¦¬ì˜¤ 1: ê¸°ë³¸ ì‹¤í–‰
    args1 = arg_parser.parse_args([])
    T.check("default: model=gpt-5.2", args1.model == "gpt-5.2")
    T.check("default: graph-rag=False", args1.graph_rag is False)
    T.check("default: graph-mode=hybrid", args1.graph_mode == "hybrid")
    T.check("default: extract-entities=False", args1.extract_entities is False)
    T.check("default: workers=3", args1.workers == 3)

    # ì‹œë‚˜ë¦¬ì˜¤ 2: Graph RAG í™œì„±í™”
    args2 = arg_parser.parse_args(["--graph-rag", "--graph-mode", "local"])
    T.check("--graph-rag activates", args2.graph_rag is True)
    T.check("--graph-mode local", args2.graph_mode == "local")

    # ì‹œë‚˜ë¦¬ì˜¤ 3: ì—”í‹°í‹° ì¶”ì¶œ + Graph RAG
    args3 = arg_parser.parse_args(["--graph-rag", "--extract-entities", "--graph-save", "/tmp/kg.json"])
    T.check("--extract-entities activates", args3.extract_entities is True)
    T.check("--graph-save custom path", args3.graph_save == "/tmp/kg.json")

    # ì‹œë‚˜ë¦¬ì˜¤ 4: Q&A only (skip ingest)
    args4 = arg_parser.parse_args(["--skip-ingest", "-q", "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸", "--graph-rag", "--graph-mode", "global"])
    T.check("--skip-ingest + question", args4.skip_ingest is True and args4.question == "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸")
    T.check("--graph-mode global", args4.graph_mode == "global")

    # ì‹œë‚˜ë¦¬ì˜¤ 5: ëª¨ë“  graph-mode ê°’ ìœ íš¨ì„±
    for mode in ["local", "global", "hybrid", "naive"]:
        args_mode = arg_parser.parse_args(["--graph-mode", mode])
        T.check(f"graph-mode '{mode}' valid", args_mode.graph_mode == mode)

    # ì‹œë‚˜ë¦¬ì˜¤ 6: ì˜ëª»ëœ graph-mode â†’ ì—ëŸ¬
    try:
        arg_parser.parse_args(["--graph-mode", "invalid"])
        T.check("invalid graph-mode raises error", False, "should have raised")
    except SystemExit:
        T.check("invalid graph-mode raises error", True)


# ==================== ë©”ì¸ ì‹¤í–‰ ====================

def _safe_run(fn, label: str):
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰ â€” ImportError/í™˜ê²½ ì˜¤ë¥˜ ì‹œì—ë„ ê³„ì† ì§„í–‰"""
    try:
        fn()
    except Exception as e:
        T.section(f"[ERR] {label}")
        T.check(f"{label} crashed", False, f"{type(e).__name__}: {e}")


def main():
    print("\n" + "=" * 70)
    print("ğŸ§ª azure_korean_doc_framework v4.0 ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("   Graph RAG | Entity Extraction | Optimized Architecture")
    print("=" * 70)

    _safe_run(test_config_v4, "Config v4.0")
    _safe_run(test_azure_clients, "Azure Clients")
    _safe_run(test_multi_model_manager, "MultiModelManager")
    _safe_run(test_parser, "Parser")
    _safe_run(test_chunker_v4, "Chunker v4.0")
    _safe_run(test_graph_rag, "Graph RAG")
    _safe_run(test_korean_tokenizer, "KoreanUnicodeTokenizer")
    _safe_run(test_entity_extractor_models, "EntityExtractor")
    _safe_run(test_agent_v4, "Agent v4.0")
    _safe_run(test_chunk_logger, "ChunkLogger")
    _safe_run(test_vector_store, "VectorStore")
    _safe_run(test_cli_args, "CLI Args")

    return T.summary()


if __name__ == "__main__":
    sys.exit(main())
