"""
azure_korean_doc_framework v4.0 í†µí•© CLI ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

ë¬¸ì„œ íŒŒì‹± â†’ ì²­í‚¹ â†’ ì¸ë±ì‹± â†’ Q&A í…ŒìŠ¤íŠ¸ë¥¼ í†µí•© ì‹¤í–‰í•©ë‹ˆë‹¤.
ë³€ê²½ ê°ì§€ ì¸ë±ì‹±, ë³‘ë ¬ ì²˜ë¦¬, Graph RAG, ì—”í‹°í‹° ì¶”ì¶œ ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.

Usage:
  python doc_chunk_main.py --path "RAG_TEST_DATA"
  python doc_chunk_main.py --path "data" --graph-rag --extract-entities
  python doc_chunk_main.py --skip-ingest -q "ì§ˆë¬¸" --model gpt-5.2
"""

import os
import json
import argparse
import glob
import hashlib
from typing import List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from azure_korean_doc_framework.parsing.parser import HybridDocumentParser
from azure_korean_doc_framework.parsing.chunker import KoreanSemanticChunker
from azure_korean_doc_framework.core.vector_store import VectorStore
from azure_korean_doc_framework.core.agent import KoreanDocAgent
from azure_korean_doc_framework.config import Config
from azure_korean_doc_framework.utils.logger import ChunkLogger

# v4.0: Graph RAG & êµ¬ì¡°í™” ì¶”ì¶œ
try:
    from azure_korean_doc_framework.core.graph_rag import KnowledgeGraphManager, QueryMode
    HAS_GRAPH_RAG = True
except ImportError:
    HAS_GRAPH_RAG = False
    print("âš ï¸ Graph RAG ë¹„í™œì„±í™” (networkx íŒ¨í‚¤ì§€ í•„ìš”: pip install networkx)")

try:
    from azure_korean_doc_framework.parsing.entity_extractor import StructuredEntityExtractor
    HAS_ENTITY_EXTRACTOR = True
except ImportError:
    HAS_ENTITY_EXTRACTOR = False

def calculate_file_hash(file_path: str) -> str:
    """íŒŒì¼ì˜ SHA256 í•´ì‹œë¥¼ ê³„ì‚°í•˜ì—¬ ë‚´ìš© ë³€ê²½ ì—¬ë¶€ë¥¼ ì •í™•íˆ íŒë‹¨í•©ë‹ˆë‹¤."""
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def process_single_file(
    file_path: str,
    parser: HybridDocumentParser,
    chunker: KoreanSemanticChunker,
    vector_store: VectorStore
) -> str:
    """
    ë‹¨ì¼ íŒŒì¼ì„ íŒŒì‹±, ì²­í‚¹, ë¡œê¹… ë° ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë‹¨ìœ„ í•¨ìˆ˜)
    """
    filename = os.path.basename(file_path)
    try:
        # 1. ë³€ê²½ ê°ì§€
        file_mod_time = os.path.getmtime(file_path)
        file_hash = calculate_file_hash(file_path)

        if vector_store.is_file_up_to_date(filename, file_mod_time, file_hash=file_hash):
             return f"â© [SKIPPED] {filename} (ìµœì‹  ìƒíƒœ)"

        # 2. íŒŒì‹± ë° ì²­í‚¹
        print(f"ğŸ”„ [START] {filename}: íŒŒì¼ ë³€ê²½ ê°ì§€. ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        vector_store.delete_documents_by_parent_id(filename)

        parsed_segments = parser.parse(file_path)

        extra_meta = {
            "source": filename,
            "last_modified": file_mod_time,
            "content_hash": file_hash
        }

        chunks = chunker.chunk(parsed_segments, filename=filename, extra_metadata=extra_meta)

        # 3. JSON ë¡œê¹… (ChunkLogger ì‚¬ìš©)
        ChunkLogger.save_chunks_to_json(chunks, filename)

        # 4. ë²¡í„° ì €ì¥ì†Œ ì—…ë¡œë“œ
        vector_store.upload_documents(chunks)
        return f"âœ… [SUCCESS] {filename}: {len(chunks)}ê°œ ì²­í¬ ì—…ë¡œë“œ ì™„ë£Œ"

    except Exception as e:
        return f"âŒ [ERROR] {filename}: {str(e)}"

def process_documents(
    target_path: str,
    parser: HybridDocumentParser,
    chunker: KoreanSemanticChunker,
    vector_store: VectorStore,
    max_workers: int = 3
):
    """
    ì§€ì •ëœ ê²½ë¡œì˜ ë¬¸ì„œë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(target_path):
        print(f"\nâ„¹ï¸ ë¬¸ì„œ ìˆ˜ì§‘ ìƒëµ: '{target_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì¸ë±ìŠ¤ëŠ” VectorStore ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ ìƒì„±ë¨ (create_index_if_not_exists)

    if os.path.isdir(target_path):
        print(f"\n--- [1ë‹¨ê³„: ë¬¸ì„œ ìˆ˜ì§‘ - {target_path} ë””ë ‰í† ë¦¬ (ë³‘ë ¬ ëª¨ë“œ)] ---")
        files_to_process = [os.path.join(target_path, f) for f in os.listdir(target_path) if f.lower().endswith('.pdf')]
    else:
        print(f"\n--- [1ë‹¨ê³„: ë¬¸ì„œ ìˆ˜ì§‘ - {target_path} íŒŒì¼] ---")
        files_to_process = [target_path] if target_path.lower().endswith('.pdf') else []

    if not files_to_process:
        print(f"â„¹ï¸ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ëŒ€ìƒ: {target_path})")
        return

    print(f"ğŸš€ ì´ {len(files_to_process)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ë³‘ë ¬ ì‘ì—… ìˆ˜: {max_workers})")

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {executor.submit(process_single_file, f, parser, chunker, vector_store): f for f in files_to_process}
        for future in as_completed(future_to_file):
            res = future.result()
            print(f"   > {res}")
            results.append(res)

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½: ì´ {len(files_to_process)}ê°œ íŒŒì¼ ì¤‘ {len([r for r in results if 'SUCCESS' in r])}ê°œ ì„±ê³µ, {len([r for r in results if 'SKIPPED' in r])}ê°œ ê±´ë„ˆëœ€")

def perform_qa_test(question: str, models: List[str]):
    """ë©€í‹° ëª¨ë¸ Q&A í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    agent = KoreanDocAgent()

    print("\n--- [2ë‹¨ê³„: ë©€í‹° ëª¨ë¸ Q&A í…ŒìŠ¤íŠ¸] ---")
    print(f"ì§ˆë¬¸: {question}")

    for model in models:
        print(f"\n--- ëª¨ë¸: {model} ---")
        answer = agent.answer_question(question, model_key=model, top_k=5)
        print(f"ë‹µë³€:\n{answer}")

def main():
    print("ğŸŒŸ Azure Korean Document Understanding & Retrieval Framework ğŸŒŸ")

    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    arg_parser = argparse.ArgumentParser(
        description="Azure Korean Document Understanding & Retrieval Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ë‹¨ì¼ íŒŒì¼ ingest
  python doc_chunk_main.py --path "RAG_TEST_DATA/sample.pdf"

  # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF ingest
  python doc_chunk_main.py --path "RAG_TEST_DATA"

  # ingestë§Œ ìˆ˜í–‰ (Q&A í…ŒìŠ¤íŠ¸ ìƒëµ)
  python doc_chunk_main.py --path "RAG_TEST_DATA" --skip-qa

  # íŠ¹ì • ì§ˆë¬¸ìœ¼ë¡œ Q&A í…ŒìŠ¤íŠ¸
  python doc_chunk_main.py --question "ì§ˆë¬¸ ë‚´ìš©"
        """
    )
    arg_parser.add_argument(
        "-p", "--path",
        type=str,
        help="Ingestí•  íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥)",
        action="append",
        default=[]
    )
    arg_parser.add_argument(
        "-q", "--question",
        type=str,
        default="ë°”ì´ì˜¤ì£¼ ì£¼ê°€ ê¸‰ë½ì— ë”°ë¥¸ ì…€íŠ¸ë¦¬ì˜¨ì˜ ì£¼ê°€ ë³€ë™ë¥ ê³¼, í˜„ëŒ€ì°¨, ì‚¼ì„±ì „ì, ì‹ í•œì§€ì£¼ì˜ ìƒìŠ¹ë¥ , ê·¸ë¦¬ê³  POSCOì™€ LGí™”í•™ì˜ í•˜ë½ë¥ ì„ ê°ê° ë§í•´ì£¼ì„¸ìš”.",
        help="Q&A í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ì§ˆë¬¸"
    )
    arg_parser.add_argument(
        "--skip-qa",
        action="store_true",
        help="Q&A í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤"
    )
    arg_parser.add_argument(
        "--skip-ingest",
        action="store_true",
        help="ë¬¸ì„œ Ingestë¥¼ ê±´ë„ˆëœë‹ˆë‹¤ (Q&Aë§Œ ìˆ˜í–‰)"
    )
    arg_parser.add_argument(
        "-w", "--workers",
        type=int,
        default=3,
        help="ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜ (ê¸°ë³¸ê°’: 3)"
    )
    arg_parser.add_argument(
        "-m", "--model",
        type=str,
        default="gpt-5.2",
        help="Q&Aì— ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-5.2)"
    )
    # v4.0: Graph RAG ì˜µì…˜
    arg_parser.add_argument(
        "--graph-rag",
        action="store_true",
        help="[v4.0] Graph RAG í™œì„±í™” (LightRAG ê¸°ë°˜ Knowledge Graph êµ¬ì¶• ë° ê²€ìƒ‰)"
    )
    arg_parser.add_argument(
        "--graph-mode",
        type=str,
        default="hybrid",
        choices=["local", "global", "hybrid", "naive"],
        help="[v4.0] Graph ê²€ìƒ‰ ëª¨ë“œ (ê¸°ë³¸ê°’: hybrid)"
    )
    arg_parser.add_argument(
        "--extract-entities",
        action="store_true",
        help="[v4.0] LangExtract ê¸°ë°˜ êµ¬ì¡°í™” ì—”í‹°í‹° ì¶”ì¶œ ìˆ˜í–‰"
    )
    arg_parser.add_argument(
        "--graph-save",
        type=str,
        default="output/knowledge_graph.json",
        help="[v4.0] Knowledge Graph ì €ì¥ ê²½ë¡œ"
    )

    args = arg_parser.parse_args()

    # 0. í™˜ê²½ ë³€ìˆ˜ ì²´í¬
    try:
        Config.validate()
    except Exception as e:
        print(e)
        return

    # 1. êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
    doc_parser = HybridDocumentParser()
    chunker = KoreanSemanticChunker()
    vector_store = VectorStore()

    # 2. ë¬¸ì„œ ìˆ˜ì§‘ (Ingestion)
    if not args.skip_ingest:
        # ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        target_paths = args.path if args.path else [r"RAG_TEST_DATA"]

        for target_path in target_paths:
            # glob íŒ¨í„´ ì²˜ë¦¬ (ì˜ˆ: RAG_TEST_DATA/*.pdf)
            if "*" in target_path or "?" in target_path:
                matched_paths = glob.glob(target_path)
                for matched_path in matched_paths:
                    if os.path.exists(matched_path):
                        process_documents(matched_path, doc_parser, chunker, vector_store, max_workers=args.workers)
            elif os.path.exists(target_path):
                process_documents(target_path, doc_parser, chunker, vector_store, max_workers=args.workers)
            else:
                print(f"âš ï¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_path}")

    # 2.5 [v4.0] Graph RAG êµ¬ì¶• (--graph-rag ì˜µì…˜)
    graph_manager = None
    if args.graph_rag and HAS_GRAPH_RAG:
        print("\n--- [Graph RAG: Knowledge Graph êµ¬ì¶•] ---")
        graph_manager = KnowledgeGraphManager(model_key=args.model)

        # ê¸°ì¡´ ê·¸ë˜í”„ ë¡œë“œ ì‹œë„
        graph_path = args.graph_save
        if os.path.exists(graph_path):
            graph_manager.load_graph(graph_path)
            print(f"ğŸ“ ê¸°ì¡´ Knowledge Graph ë¡œë“œ ì™„ë£Œ")

        # ìƒˆë¡œìš´ ì²­í¬ê°€ ìˆìœ¼ë©´ ê·¸ë˜í”„ êµ¬ì¶•
        if not args.skip_ingest:
            chunk_files = glob.glob("output/*_chunks.json")
            if chunk_files:
                all_chunk_texts = []
                for cf in chunk_files:
                    with open(cf, 'r', encoding='utf-8') as f:
                        chunks_data = json.load(f)
                    for c in chunks_data:
                        if not c.get('metadata', {}).get('is_table_data'):
                            all_chunk_texts.append({"page_content": c.get('page_content', '')})

                if all_chunk_texts:
                    print(f"ğŸ” {len(all_chunk_texts)}ê°œ í…ìŠ¤íŠ¸ ì²­í¬ì—ì„œ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ ì¤‘...")
                    graph_manager.extract_from_chunks(
                        all_chunk_texts,
                        batch_size=Config.GRAPH_ENTITY_BATCH_SIZE,
                    )
                    # ê·¸ë˜í”„ ì €ì¥
                    os.makedirs(os.path.dirname(graph_path) or '.', exist_ok=True)
                    graph_manager.save_graph(graph_path)

        stats = graph_manager.get_stats()
        print(f"ğŸ“Š Knowledge Graph í†µê³„: ë…¸ë“œ {stats['nodes']}ê°œ, ì—£ì§€ {stats['edges']}ê°œ")
        if stats.get('entity_types'):
            for et, count in stats['entity_types'].items():
                print(f"   - {et}: {count}ê°œ")

    # 2.6 [v4.0] êµ¬ì¡°í™” ì—”í‹°í‹° ì¶”ì¶œ (--extract-entities ì˜µì…˜)
    if args.extract_entities and HAS_ENTITY_EXTRACTOR:
        print("\n--- [v4.0: êµ¬ì¡°í™” ì—”í‹°í‹° ì¶”ì¶œ (LangExtract ê¸°ë°˜)] ---")
        extractor = StructuredEntityExtractor(
            model_key=args.model,
            extraction_passes=Config.EXTRACTION_PASSES,
            max_chunk_chars=Config.EXTRACTION_MAX_CHUNK_CHARS,
            max_workers=Config.EXTRACTION_MAX_WORKERS,
        )

        chunk_files = glob.glob("output/*_chunks.json")
        for cf in chunk_files:
            with open(cf, 'r', encoding='utf-8') as f:
                chunks_data = json.load(f)

            texts = [c.get('page_content', '') for c in chunks_data if c.get('page_content')]
            full_text = "\n\n".join(texts[:20])  # ìƒìœ„ 20ê°œ ì²­í¬ë§Œ

            result = extractor.extract(full_text)
            output_path = cf.replace('_chunks.json', '_entities.json')
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(extractor.extractions_to_dict(result), f, ensure_ascii=False, indent=2)
            print(f"   âœ… {os.path.basename(cf)}: {len(result.extractions)}ê°œ ì—”í‹°í‹° ì¶”ì¶œ â†’ {output_path}")

    # 3. Q&A í…ŒìŠ¤íŠ¸
    if not args.skip_qa:
        models_to_test = [args.model]

        # v4.0: Graph RAGê°€ í™œì„±í™”ë˜ë©´ graph_enhanced_answer ì‚¬ìš©
        if graph_manager and graph_manager.graph.number_of_nodes() > 0:
            print("\n--- [2ë‹¨ê³„: Graph-Enhanced Q&A í…ŒìŠ¤íŠ¸] ---")
            agent = KoreanDocAgent(graph_manager=graph_manager)
            print(f"ì§ˆë¬¸: {args.question}")
            for model in models_to_test:
                print(f"\n--- ëª¨ë¸: {model} (Graph-Enhanced, mode={args.graph_mode}) ---")
                answer = agent.graph_enhanced_answer(
                    args.question,
                    model_key=model,
                    top_k=5,
                    graph_query_mode=args.graph_mode,
                )
                print(f"ë‹µë³€:\n{answer}")
        else:
            perform_qa_test(args.question, models_to_test)

if __name__ == "__main__":
    main()
