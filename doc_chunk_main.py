import os
import argparse
import glob
import hashlib
from typing import List, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from azure_korean_doc_framework.parsing.parser import HybridDocumentParser
from azure_korean_doc_framework.parsing.chunker import KoreanSemanticChunker
from azure_korean_doc_framework.core.vector_store import VectorStore
from azure_korean_doc_framework.core.agent import KoreanDocAgent
from azure_korean_doc_framework.config import Config
from azure_korean_doc_framework.utils.logger import ChunkLogger

def calculate_file_hash(file_path: str) -> str:
    """íŒŒì¼ì˜ SHA256 í•´ì‹œë¥¼ ê³„ì‚°í•˜ì—¬ ë‚´ìš© ë³€ê²½ ì—¬ë¶€ë¥¼ ì •í™•íˆ íŒë‹¨í•©ë‹ˆë‹¤."""
    sha256_hash = hashlib.sha256()
    with open(file_path, "rb") as f:
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def process_single_file(
    file_path: str,
    parser: HybridDocumentParser,
    chunker: KoreanSemanticChunker,
    vector_store: VectorStore
) -> str:
    """
    ë‹¨ì¼ íŒŒì¼ì„ íŒŒì‹±, ì²­í‚¹, ë¡œê¹… ë° ì—…ë¡œë“œí•©ë‹ˆë‹¤.
    (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë‹¨ìœ„ í•¨ìˆ˜)
    """
    filename = os.path.basename(file_path)
    try:
        # 1. ë³€ê²½ ê°ì§€
        file_mod_time = os.path.getmtime(file_path)
        file_hash = calculate_file_hash(file_path)

        if vector_store.is_file_up_to_date(filename, file_mod_time, file_hash=file_hash):
             return f"â© [SKIPPED] {filename} (ìµœì‹  ìƒíƒœ)"

        # 2. íŒŒì‹± ë° ì²­í‚¹
        print(f"ğŸ”„ [START] {filename}: íŒŒì¼ ë³€ê²½ ê°ì§€. ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        vector_store.delete_documents_by_parent_id(filename)

        parsed_segments = parser.parse(file_path)

        extra_meta = {
            "source": filename,
            "last_modified": file_mod_time,
            "content_hash": file_hash
        }

        chunks = chunker.chunk(parsed_segments, filename=filename, extra_metadata=extra_meta)

        # 3. JSON ë¡œê¹… (ChunkLogger ì‚¬ìš©)
        ChunkLogger.save_chunks_to_json(chunks, filename)

        # 4. ë²¡í„° ì €ì¥ì†Œ ì—…ë¡œë“œ
        vector_store.upload_documents(chunks)
        return f"âœ… [SUCCESS] {filename}: {len(chunks)}ê°œ ì²­í¬ ì—…ë¡œë“œ ì™„ë£Œ"

    except Exception as e:
        return f"âŒ [ERROR] {filename}: {str(e)}"

def process_documents(
    target_path: str,
    parser: HybridDocumentParser,
    chunker: KoreanSemanticChunker,
    vector_store: VectorStore,
    max_workers: int = 3
):
    """
    ì§€ì •ëœ ê²½ë¡œì˜ ë¬¸ì„œë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    if not os.path.exists(target_path):
        print(f"\nâ„¹ï¸ ë¬¸ì„œ ìˆ˜ì§‘ ìƒëµ: '{target_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì¸ë±ìŠ¤ëŠ” VectorStore ì´ˆê¸°í™” ì‹œ ìë™ìœ¼ë¡œ ìƒì„±ë¨ (create_index_if_not_exists)

    if os.path.isdir(target_path):
        print(f"\n--- [1ë‹¨ê³„: ë¬¸ì„œ ìˆ˜ì§‘ - {target_path} ë””ë ‰í† ë¦¬ (ë³‘ë ¬ ëª¨ë“œ)] ---")
        files_to_process = [os.path.join(target_path, f) for f in os.listdir(target_path) if f.lower().endswith('.pdf')]
    else:
        print(f"\n--- [1ë‹¨ê³„: ë¬¸ì„œ ìˆ˜ì§‘ - {target_path} íŒŒì¼] ---")
        files_to_process = [target_path] if target_path.lower().endswith('.pdf') else []

    if not files_to_process:
        print(f"â„¹ï¸ ì²˜ë¦¬í•  PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (ëŒ€ìƒ: {target_path})")
        return

    print(f"ğŸš€ ì´ {len(files_to_process)}ê°œì˜ íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. (ë³‘ë ¬ ì‘ì—… ìˆ˜: {max_workers})")

    # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬
    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {executor.submit(process_single_file, f, parser, chunker, vector_store): f for f in files_to_process}
        for future in as_completed(future_to_file):
            res = future.result()
            print(f"   > {res}")
            results.append(res)

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ ìš”ì•½: ì´ {len(files_to_process)}ê°œ íŒŒì¼ ì¤‘ {len([r for r in results if 'SUCCESS' in r])}ê°œ ì„±ê³µ, {len([r for r in results if 'SKIPPED' in r])}ê°œ ê±´ë„ˆëœ€")

def perform_qa_test(question: str, models: List[str]):
    """ë©€í‹° ëª¨ë¸ Q&A í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    agent = KoreanDocAgent()

    print("\n--- [2ë‹¨ê³„: ë©€í‹° ëª¨ë¸ Q&A í…ŒìŠ¤íŠ¸] ---")
    print(f"ì§ˆë¬¸: {question}")

    for model in models:
        print(f"\n--- ëª¨ë¸: {model} ---")
        answer = agent.answer_question(question, model_key=model, top_k=5)
        print(f"ë‹µë³€:\n{answer}")

def main():
    print("ğŸŒŸ Azure Korean Document Understanding & Retrieval Framework ğŸŒŸ")

    # ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±
    arg_parser = argparse.ArgumentParser(
        description="Azure Korean Document Understanding & Retrieval Framework",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ë‹¨ì¼ íŒŒì¼ ingest
  python doc_chunk_main.py --path "RAG_TEST_DATA/sample.pdf"

  # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  PDF ingest
  python doc_chunk_main.py --path "RAG_TEST_DATA"

  # ingestë§Œ ìˆ˜í–‰ (Q&A í…ŒìŠ¤íŠ¸ ìƒëµ)
  python doc_chunk_main.py --path "RAG_TEST_DATA" --skip-qa

  # íŠ¹ì • ì§ˆë¬¸ìœ¼ë¡œ Q&A í…ŒìŠ¤íŠ¸
  python doc_chunk_main.py --question "ì§ˆë¬¸ ë‚´ìš©"
        """
    )
    arg_parser.add_argument(
        "-p", "--path",
        type=str,
        help="Ingestí•  íŒŒì¼ ë˜ëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (ì—¬ëŸ¬ ê°œ ì§€ì • ê°€ëŠ¥)",
        action="append",
        default=[]
    )
    arg_parser.add_argument(
        "-q", "--question",
        type=str,
        default="ë°”ì´ì˜¤ì£¼ ì£¼ê°€ ê¸‰ë½ì— ë”°ë¥¸ ì…€íŠ¸ë¦¬ì˜¨ì˜ ì£¼ê°€ ë³€ë™ë¥ ê³¼, í˜„ëŒ€ì°¨, ì‚¼ì„±ì „ì, ì‹ í•œì§€ì£¼ì˜ ìƒìŠ¹ë¥ , ê·¸ë¦¬ê³  POSCOì™€ LGí™”í•™ì˜ í•˜ë½ë¥ ì„ ê°ê° ë§í•´ì£¼ì„¸ìš”.",
        help="Q&A í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  ì§ˆë¬¸"
    )
    arg_parser.add_argument(
        "--skip-qa",
        action="store_true",
        help="Q&A í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤"
    )
    arg_parser.add_argument(
        "--skip-ingest",
        action="store_true",
        help="ë¬¸ì„œ Ingestë¥¼ ê±´ë„ˆëœë‹ˆë‹¤ (Q&Aë§Œ ìˆ˜í–‰)"
    )
    arg_parser.add_argument(
        "-w", "--workers",
        type=int,
        default=3,
        help="ë³‘ë ¬ ì²˜ë¦¬ ì‘ì—… ìˆ˜ (ê¸°ë³¸ê°’: 3)"
    )
    arg_parser.add_argument(
        "-m", "--model",
        type=str,
        default="gpt-5.2",
        help="Q&Aì— ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: gpt-5.2)"
    )

    args = arg_parser.parse_args()

    # 0. í™˜ê²½ ë³€ìˆ˜ ì²´í¬
    try:
        Config.validate()
    except Exception as e:
        print(e)
        return

    # 1. êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
    doc_parser = HybridDocumentParser()
    chunker = KoreanSemanticChunker()
    vector_store = VectorStore()

    # 2. ë¬¸ì„œ ìˆ˜ì§‘ (Ingestion)
    if not args.skip_ingest:
        # ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
        target_paths = args.path if args.path else [r"RAG_TEST_DATA"]

        for target_path in target_paths:
            # glob íŒ¨í„´ ì²˜ë¦¬ (ì˜ˆ: RAG_TEST_DATA/*.pdf)
            if "*" in target_path or "?" in target_path:
                matched_paths = glob.glob(target_path)
                for matched_path in matched_paths:
                    if os.path.exists(matched_path):
                        process_documents(matched_path, doc_parser, chunker, vector_store, max_workers=args.workers)
            elif os.path.exists(target_path):
                process_documents(target_path, doc_parser, chunker, vector_store, max_workers=args.workers)
            else:
                print(f"âš ï¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_path}")

    # 3. Q&A í…ŒìŠ¤íŠ¸
    if not args.skip_qa:
        models_to_test = [args.model]
        perform_qa_test(args.question, models_to_test)

if __name__ == "__main__":
    main()
