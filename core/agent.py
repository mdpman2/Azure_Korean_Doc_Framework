from typing import List, Tuple, Optional, Union, Any
from azure.search.documents.models import VectorizedQuery
from langchain_openai import AzureOpenAIEmbeddings
from .multi_model_manager import MultiModelManager
from ..utils.azure_clients import AzureClientFactory
from ..config import Config

class KoreanDocAgent:
    """
    í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ ë° Q&A ì „ë¬¸ê°€ ê²€ìƒ‰ ì—ì´ì „íŠ¸.

    Azure AI Searchì˜ Hybrid Searchë¥¼ í™œìš©í•˜ì—¬ ë¬¸ë§¥ì„ ì°¾ê³ ,
    Azure OpenAI ëª¨ë¸ë“¤ì„ í†µí•´ ì§€ëŠ¥ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """

    def __init__(self, model_key: Optional[str] = None):
        """
        KoreanDocAgentë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            model_key: ë‹µë³€ ìƒì„± ì‹œ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•  ëª¨ë¸ í‚¤ (Config.MODELSì— ì •ì˜ëœ í‚¤).
        """
        self.model_manager = MultiModelManager(default_model=model_key)
        self.search_client = AzureClientFactory.get_search_client()

        # ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (ë²¡í„° ê²€ìƒ‰ìš©)
        self.embeddings = AzureOpenAIEmbeddings(
            azure_deployment=Config.EMBEDDING_DEPLOYMENT,
            openai_api_version=Config.OPENAI_API_VERSION,
            azure_endpoint=Config.OPENAI_ENDPOINT,
            api_key=Config.OPENAI_API_KEY
        )

    def answer_question(
        self,
        question: str,
        model_key: Optional[str] = None,
        return_context: bool = False,
        top_k: int = 5
    ) -> Union[str, Tuple[str, List[str]]]:
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        1. AI Searchì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„°+í‚¤ì›Œë“œ) ë° ì‹œë§¨í‹± ë­í‚¹ ìˆ˜í–‰
        2. ê²€ìƒ‰ëœ ë¬¸ë§¥(Context)ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìƒì„± (ì¶œì²˜ ì •ë³´ í¬í•¨)

        Args:
            question: ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë¬¸ìì—´.
            model_key: ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  íŠ¹ì • ëª¨ë¸ í‚¤.
            return_context: Trueì¼ ê²½ìš° ë‹µë³€ê³¼ í•¨ê»˜ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œì˜ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5).

        Returns:
            ë‹µë³€ ë¬¸ìì—´ ë˜ëŠ” (ë‹µë³€, ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸) íŠœí”Œ.
        """
        print(f"ğŸ” Searching for: {question} (top_k={top_k})")

        # 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„± (ë²¡í„° ê²€ìƒ‰ìš©)
        query_vector = self.embeddings.embed_query(question)
        vector_query = VectorizedQuery(vector=query_vector, k_nearest_neighbors=50, fields="text_vector")

        # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° ì‹œë§¨í‹± ë­í‚¹ ìˆ˜í–‰
        try:
            results = self.search_client.search(
                search_text=question,
                vector_queries=[vector_query],
                select=["chunk", "parent_id"],
                query_type="semantic",
                semantic_configuration_name="my-semantic-config",
                top=top_k
            )

            contexts = []
            sources = set()

            for r in results:
                content = r.get('chunk') or r.get('content') or ""
                source = r.get('parent_id') or "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"

                if content:
                    # ì»¨í…ìŠ¤íŠ¸ì— ì¶œì²˜ ì •ë³´ ëª…ì‹œì ìœ¼ë¡œ ì‚½ì…
                    contexts.append(f"[ì¶œì²˜: {source}]\n{content}")
                    sources.add(source)

        except Exception as e:
            print(f"   âŒ Search failed: {e}")
            contexts = []

        context_str = "\n\n".join(contexts)

        if not context_str:
            print("   âš ï¸ No relevant documentation found.")
            context_str = "ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        system_prompt = (
            "ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ë° Q&A ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ [Context] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ [Question]ì— í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
            "\n\n### ë‹µë³€ ê·œì¹™:"
            "\n1. ë‹µë³€ ì‹œ ë°˜ë“œì‹œ í•´ë‹¹ ì •ë³´ì˜ **ì¶œì²˜(íŒŒì¼ëª…)**ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”. (ì˜ˆ: '...ì…ë‹ˆë‹¤ [ì¶œì²˜: íŒŒì¼ëª….pdf]')"
            "\n2. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì·¨í•©í•œ ê²½ìš°, ê°ê°ì˜ ì¶œì²˜ë¥¼ ë°íˆì„¸ìš”."
            "\n3. ì¶”ì¶œëœ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì•„ëŠ” ë²”ìœ„ ë‚´ì—ì„œ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•˜ë˜, ì •ë³´ê°€ ì „í˜€ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”."
        )

        user_prompt = f"[Context]\n{context_str}\n\n[Question]\n{question}"

        # LLM í˜¸ì¶œì„ í†µí•œ ë‹µë³€ ìƒì„±
        answer = self.model_manager.get_completion(
            prompt=user_prompt,
            model_key=model_key,
            system_message=system_prompt
        )

        if return_context:
            return answer, contexts
        return answer
