from azure.search.documents.indexes.models import (
    SearchIndex,
    SimpleField,
    SearchField,
    SearchFieldDataType,
    VectorSearch,
    HnswAlgorithmConfiguration,
    VectorSearchProfile,
    SemanticConfiguration,
    SemanticPrioritizedFields,
    SemanticField,
    SemanticSearch
)
from ..utils.azure_clients import AzureClientFactory
from ..config import Config

class VectorStore:
    """
    Azure AI Search ê¸°ë°˜ì˜ ë²¡í„° ì €ì¥ì†Œ ê´€ë¦¬ í´ë˜ìŠ¤
    ì¸ë±ìŠ¤ ìƒì„±, ë¬¸ì„œ ë²¡í„°í™”(ì„ë² ë”©), ê²€ìƒ‰ ë° ì¦ë¶„ ì—…ë°ì´íŠ¸ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    """
    def __init__(self, index_name=None):
        self.index_name = index_name or Config.SEARCH_INDEX_NAME
        self.index_client = AzureClientFactory.get_search_index_client()
        self.search_client = AzureClientFactory.get_search_client(self.index_name)

        # ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì„¤ì • (LangChain AzureOpenAIEmbeddings ì‚¬ìš©)
        from langchain_openai import AzureOpenAIEmbeddings
        self.embeddings = AzureOpenAIEmbeddings(
            azure_deployment=Config.EMBEDDING_DEPLOYMENT,
            openai_api_version=Config.OPENAI_API_VERSION,
            azure_endpoint=Config.OPENAI_ENDPOINT,
            api_key=Config.OPENAI_API_KEY
        )

    def create_index_if_not_exists(self, vector_dim=1536):
        """
        AI Search ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±í•©ë‹ˆë‹¤.
        ë²¡í„° ê²€ìƒ‰ ë° ì‹œë§¨í‹± ë­í‚¹(Semantic Ranking) ì„¤ì •ì„ í¬í•¨í•©ë‹ˆë‹¤.
        """
        try:
            self.index_client.get_index(self.index_name)
            print(f"âœ… ì¸ë±ìŠ¤ ì¡´ì¬: '{self.index_name}'")
        except:
            print(f"ğŸ› ï¸ ì¸ë±ìŠ¤ ìƒì„± ì¤‘: '{self.index_name}'...")

            fields = [
                SimpleField(name="chunk_id", type=SearchFieldDataType.String, key=True),
                SimpleField(name="parent_id", type=SearchFieldDataType.String, filterable=True),
                SimpleField(name="last_modified", type=SearchFieldDataType.String, filterable=True),
                SimpleField(name="content_hash", type=SearchFieldDataType.String, filterable=True),
                SearchField(name="chunk", type=SearchFieldDataType.String, searchable=True, analyzer_name="ko.microsoft"),
                SearchField(name="title", type=SearchFieldDataType.String, searchable=True),
                SearchField(name="text_vector", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                            searchable=True, vector_search_dimensions=vector_dim, vector_search_profile_name="my-vector-profile"),
            ]

            # ë²¡í„° ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ë° í”„ë¡œí•„ ì„¤ì •
            vector_search = VectorSearch(
                algorithms=[HnswAlgorithmConfiguration(name="my-hnsw")],
                profiles=[VectorSearchProfile(name="my-vector-profile", algorithm_configuration_name="my-hnsw")]
            )

            # ì‹œë§¨í‹± ê²€ìƒ‰ ì„¤ì • (í•œêµ­ì–´ ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ)
            semantic_search = SemanticSearch(
                configurations=[
                    SemanticConfiguration(
                        name="my-semantic-config",
                        prioritized_fields=SemanticPrioritizedFields(
                            title_field=None,
                            content_fields=[SemanticField(field_name="chunk")]
                        )
                    )
                ]
            )

            index = SearchIndex(
                name=self.index_name,
                fields=fields,
                vector_search=vector_search,
                semantic_search=semantic_search
            )
            self.index_client.create_index(index)
            print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: '{self.index_name}'")

        # ê¸°ì¡´ ì¸ë±ìŠ¤ê°€ ìˆë”ë¼ë„ í•„ìš”í•œ í•„ë“œ(ì¦ë¶„ ì—…ë°ì´íŠ¸ìš©)ê°€ ëˆ„ë½ë˜ì—ˆëŠ”ì§€ í™•ì¸
        self._ensure_incremental_fields()

    def _ensure_incremental_fields(self):
        """ê¸°ì¡´ ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆì— ì¦ë¶„ ì—…ë°ì´íŠ¸ìš© í•„ë“œ(last_modified, content_hash)ê°€ ì—†ìœ¼ë©´ ë™ì ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            index = self.index_client.get_index(self.index_name)
            field_names = [f.name for f in index.fields]
            updated = False

            if "last_modified" not in field_names:
                print(f"ğŸ› ï¸ 'last_modified' í•„ë“œ ì¶”ê°€ ì¤‘: {self.index_name}")
                index.fields.append(SimpleField(name="last_modified", type=SearchFieldDataType.String, filterable=True))
                updated = True

            if "content_hash" not in field_names:
                print(f"ğŸ› ï¸ 'content_hash' í•„ë“œ ì¶”ê°€ ì¤‘: {self.index_name}")
                index.fields.append(SimpleField(name="content_hash", type=SearchFieldDataType.String, filterable=True))
                updated = True

            if updated:
                self.index_client.create_or_update_index(index)
                print("âœ… ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ ì™„ë£Œ.")
        except Exception as e:
            print(f"âš ï¸ ì¸ë±ìŠ¤ ìŠ¤í‚¤ë§ˆ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

    def upload_documents(self, chunks):
        """
        ë¬¸ì„œ ì²­í¬ë¥¼ ë²¡í„°í™”í•˜ì—¬ AI Searchì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.
        ë°°ì¹˜ ì„ë² ë”©ì„ í†µí•´ API í˜¸ì¶œ íšŸìˆ˜ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
        """
        if not chunks:
            return

        print(f"ğŸ“¡ {len(chunks)}ê°œ ì²­í¬ ë°°ì¹˜ ì„ë² ë”© ë° ì—…ë¡œë“œ ì¤‘... ì¸ë±ìŠ¤: '{self.index_name}'")

        # 1. ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í•œêº¼ë²ˆì— ì„ë² ë”© (ì„±ëŠ¥ ìµœì í™” í•µì‹¬)
        texts = [chunk.page_content for chunk in chunks]
        vectors = self.embeddings.embed_documents(texts)

        # 2. AI Search ì—…ë¡œë“œìš© ë°ì´í„° êµ¬ì„±
        documents = []
        for i, (chunk, vector) in enumerate(zip(chunks, vectors)):
            documents.append({
                "chunk_id": f"chunk_{i}",
                "chunk": chunk.page_content,
                "title": chunk.metadata.get("Header 1", "No Title"),
                "parent_id": str(chunk.metadata.get("source", "unknown")),
                "last_modified": str(chunk.metadata.get("last_modified", "")),
                "content_hash": str(chunk.metadata.get("content_hash", "")),
                "text_vector": vector
            })

        # 3. 50ê°œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì—…ë¡œë“œ
        for j in range(0, len(documents), 50):
            batch = documents[j:j+50]
            self.search_client.upload_documents(batch)

        print(f"âœ… {len(documents)}ê°œ ë¬¸ì„œ ì—…ë¡œë“œ ì™„ë£Œ.")

    def is_file_up_to_date(self, file_name, file_mod_time, file_hash=None):
        """
        í•´ë‹¹ íŒŒì¼ì´ ì´ë¯¸ ìµœì‹  ìƒíƒœë¡œ ì¸ë±ì‹±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
        í•´ì‹œ(ë‚´ìš© ê²€ì‚¬)ë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™•ì¸í•˜ê³ , ë³´ì¡°ì ìœ¼ë¡œ ìˆ˜ì • ì‹œê°„ì„ í™•ì¸í•©ë‹ˆë‹¤.
        """
        try:
            results = self.search_client.search(
                search_text="*",
                filter=f"parent_id eq '{file_name}'",
                select=["last_modified", "content_hash"],
                top=1
            )
            for r in results:
                # 1. íŒŒì¼ ë‚´ìš© í•´ì‹œ ë¹„êµ (ê°€ì¥ ì •í™•í•œ ë°©ë²•)
                if file_hash:
                    stored_hash = r.get("content_hash")
                    if stored_hash == file_hash:
                        print(f"â­ï¸ ì¤‘ë³µ ìŠ¤í‚µ: '{file_name}' (ë‚´ìš©ì´ ë™ì¼í•¨)")
                        return True

                # 2. ìˆ˜ì • ì‹œê°„ ë¹„êµ
                stored_time = r.get("last_modified")
                if stored_time and float(stored_time) >= file_mod_time:
                    print(f"â­ï¸ ì¤‘ë³µ ìŠ¤í‚µ: '{file_name}' (ë‚ ì§œê°€ ìµœì‹ ì„)")
                    return True
            return False
        except Exception:
            return False

    def delete_documents_by_parent_id(self, parent_id):
        """íŠ¹ì • íŒŒì¼(parent_id)ì— ì—°ê´€ëœ ëª¨ë“  ì²­í¬ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤ (ì—…ë°ì´íŠ¸ ì „ ì²˜ë¦¬ìš©)."""
        try:
            results = self.search_client.search(
                search_text="*",
                filter=f"parent_id eq '{parent_id}'",
                select=["chunk_id"]
            )

            ids_to_delete = [{"chunk_id": r["chunk_id"]} for r in results]
            if ids_to_delete:
                print(f"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì¤‘: '{parent_id}' ({len(ids_to_delete)}ê°œ ì²­í¬)")
                for i in range(0, len(ids_to_delete), 100):
                    batch = ids_to_delete[i:i+100]
                    self.search_client.delete_documents(batch)
                print("âœ… ì‚­ì œ ì™„ë£Œ.")
        except Exception as e:
            print(f"âš ï¸ ë°ì´í„° ì‚­ì œ ì˜¤ë¥˜: {e}")
