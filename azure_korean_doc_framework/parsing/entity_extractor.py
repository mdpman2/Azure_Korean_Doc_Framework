"""
LangExtract-inspired êµ¬ì¡°í™”ëœ ì—”í‹°í‹° ì¶”ì¶œ ëª¨ë“ˆ (v4.0)

Google LangExtract(https://github.com/google/langextract)ì˜ í•µì‹¬ ê°œë…ì„ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„í•œ
í•œêµ­ì–´ ë¬¸ì„œ êµ¬ì¡°í™” ì¶”ì¶œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- GPT-5.2ë¥¼ í™œìš©í•œ Few-Shot ê¸°ë°˜ êµ¬ì¡°í™” ì¶”ì¶œ
- í•œêµ­ì–´/CJK Unicode í† í¬ë‚˜ì´ì € (ì •í™•í•œ ìœ„ì¹˜ ë§¤í•‘)
- Multi-Pass ì¶”ì¶œ (í–¥ìƒëœ Recall)
- Source Grounding (ì›ë¬¸ ìœ„ì¹˜ ì¶”ì )
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›

[2026-07 v4.0 ì‹ ê·œ]
- LangExtract ê¸°ë°˜ êµ¬ì¡°í™” ì¶”ì¶œ ì•„í‚¤í…ì²˜
- í•œêµ­ì–´ Unicode í† í¬ë‚˜ì´ì €
- ì›ë¬¸ ìœ„ì¹˜ ì¶”ì  (char_interval)
"""

import re
import json
import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..config import Config
from ..utils.azure_clients import AzureClientFactory


# ==================== ë°ì´í„° ëª¨ë¸ ====================

@dataclass
class CharInterval:
    """ì›ë¬¸ì—ì„œì˜ ë¬¸ì ìœ„ì¹˜ êµ¬ê°„"""
    start_pos: int
    end_pos: int

    @property
    def length(self) -> int:
        return self.end_pos - self.start_pos


@dataclass
class Extraction:
    """
    ì¶”ì¶œëœ ì—”í‹°í‹° (LangExtractì˜ Extraction í´ë˜ìŠ¤ ì°¸ì¡°)

    Attributes:
        extraction_class: ì—”í‹°í‹° í´ë˜ìŠ¤ (ì˜ˆ: "ì¸ë¬¼", "ì¡°ì§")
        extraction_text: ì¶”ì¶œëœ ì›ë¬¸ í…ìŠ¤íŠ¸
        char_interval: ì›ë¬¸ì—ì„œì˜ ìœ„ì¹˜
        attributes: ì¶”ê°€ ì†ì„± (ì˜ˆ: {"ì—­í• ": "ëŒ€í‘œì´ì‚¬"})
        description: ì—”í‹°í‹° ì„¤ëª…
    """
    extraction_class: str
    extraction_text: str
    char_interval: Optional[CharInterval] = None
    attributes: Dict[str, str] = field(default_factory=dict)
    description: str = ""
    alignment_status: str = "aligned"  # aligned, fuzzy, unaligned


@dataclass
class ExampleData:
    """
    Few-Shot ì˜ˆì‹œ ë°ì´í„° (LangExtractì˜ ExampleData ì°¸ì¡°)

    LLMì—ê²Œ ì¶”ì¶œ íŒ¨í„´ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œì…ë‹ˆë‹¤.
    """
    text: str
    extractions: List[Extraction] = field(default_factory=list)


@dataclass
class ExtractionResult:
    """ì¶”ì¶œ ê²°ê³¼"""
    text: str
    extractions: List[Extraction] = field(default_factory=list)
    processing_time: float = 0.0
    num_chunks: int = 0
    num_passes: int = 1


# ==================== í•œêµ­ì–´ Unicode í† í¬ë‚˜ì´ì € ====================

# CJK/Hangul íŒ¨í„´ (LangExtractì˜ _CJK_PATTERN ì°¸ì¡°)
_HANGUL_PATTERN = re.compile(r'[\uAC00-\uD7AF\u1100-\u11FF\u3130-\u318F\uA960-\uA97F\uD7B0-\uD7FF]')
_CJK_PATTERN = re.compile(r'[\u4E00-\u9FFF\u3400-\u4DBF\uF900-\uFAFF]')


class KoreanUnicodeTokenizer:
    """
    í•œêµ­ì–´/CJK ë¬¸ìë¥¼ ìœ„í•œ Unicode ê¸°ë°˜ í† í¬ë‚˜ì´ì €

    LangExtractì˜ UnicodeTokenizerë¥¼ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„:
    - í•œêµ­ì–´ ìëª¨/ìŒì ˆ ë‹¨ìœ„ ë¶„ë¦¬
    - CJK ë¬¸ì ê°œë³„ í† í° ì²˜ë¦¬
    - ì •í™•í•œ char_interval ê³„ì‚°

    ì°¸ì¡°: https://github.com/google/langextract (Japanese extraction example)
    """

    @staticmethod
    def find_text_positions(
        source_text: str,
        search_text: str,
        fuzzy: bool = True,
    ) -> List[CharInterval]:
        """
        ì›ë¬¸ì—ì„œ ê²€ìƒ‰ í…ìŠ¤íŠ¸ì˜ ì •í™•í•œ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

        Args:
            source_text: ì „ì²´ ì›ë¬¸
            search_text: ì°¾ì„ í…ìŠ¤íŠ¸
            fuzzy: í¼ì§€ ë§¤ì¹­ í—ˆìš© ì—¬ë¶€

        Returns:
            CharInterval ë¦¬ìŠ¤íŠ¸
        """
        positions = []

        # 1. ì •í™•í•œ ë§¤ì¹­
        start = 0
        while True:
            idx = source_text.find(search_text, start)
            if idx == -1:
                break
            positions.append(CharInterval(
                start_pos=idx,
                end_pos=idx + len(search_text),
            ))
            start = idx + 1

        # 2. ì •í™•í•œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ í¼ì§€ ë§¤ì¹­ ì‹œë„
        if not positions and fuzzy:
            # ê³µë°± ì •ê·œí™” í›„ ì¬ì‹œë„
            normalized_source = re.sub(r'\s+', ' ', source_text)
            normalized_search = re.sub(r'\s+', ' ', search_text)

            idx = normalized_source.find(normalized_search)
            if idx != -1:
                # ì›ë¬¸ ìœ„ì¹˜ë¡œ ì—­ë§¤í•‘
                orig_idx = _map_normalized_to_original(source_text, idx)
                if orig_idx >= 0:
                    positions.append(CharInterval(
                        start_pos=orig_idx,
                        end_pos=orig_idx + len(search_text),
                    ))

        return positions

    @staticmethod
    def is_hangul(char: str) -> bool:
        """í•œê¸€ ìŒì ˆ/ìëª¨ ì—¬ë¶€ í™•ì¸"""
        return bool(_HANGUL_PATTERN.match(char))

    @staticmethod
    def count_hangul_ratio(text: str) -> float:
        """í…ìŠ¤íŠ¸ì˜ í•œê¸€ ë¹„ìœ¨ ê³„ì‚°"""
        if not text:
            return 0.0
        hangul_count = sum(1 for c in text if _HANGUL_PATTERN.match(c))
        return hangul_count / len(text)


def _map_normalized_to_original(original: str, normalized_pos: int) -> int:
    """ì •ê·œí™”ëœ ìœ„ì¹˜ë¥¼ ì›ë¬¸ ìœ„ì¹˜ë¡œ ë§¤í•‘"""
    norm_idx = 0
    orig_idx = 0
    in_whitespace = False

    while orig_idx < len(original) and norm_idx < normalized_pos:
        if original[orig_idx].isspace():
            if not in_whitespace:
                norm_idx += 1
                in_whitespace = True
        else:
            norm_idx += 1
            in_whitespace = False
        orig_idx += 1

    return orig_idx


# ==================== êµ¬ì¡°í™” ì¶”ì¶œê¸° ====================

# ê¸°ë³¸ í•œêµ­ì–´ Few-Shot ì˜ˆì‹œ
DEFAULT_KOREAN_EXAMPLES = [
    ExampleData(
        text="ì‚¼ì„±ì „ì ì´ì¬ìš© íšŒì¥ì´ 2025ë…„ ë°˜ë„ì²´ íˆ¬ì í™•ëŒ€ ê³„íšì„ ë°œí‘œí–ˆë‹¤.",
        extractions=[
            Extraction(
                extraction_class="ì¡°ì§",
                extraction_text="ì‚¼ì„±ì „ì",
                attributes={"ì‚°ì—…": "ë°˜ë„ì²´/ì „ì"},
            ),
            Extraction(
                extraction_class="ì¸ë¬¼",
                extraction_text="ì´ì¬ìš©",
                attributes={"ì§í•¨": "íšŒì¥", "ì†Œì†": "ì‚¼ì„±ì „ì"},
            ),
            Extraction(
                extraction_class="ì‚¬ê±´",
                extraction_text="ë°˜ë„ì²´ íˆ¬ì í™•ëŒ€ ê³„íšì„ ë°œí‘œ",
                attributes={"ì‹œê¸°": "2025ë…„", "ë¶„ì•¼": "ë°˜ë„ì²´"},
            ),
        ],
    ),
]

EXTRACTION_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

### ì¶”ì¶œ ê·œì¹™
{prompt_description}

### ì¤‘ìš” ê·œì¹™
1. extraction_textëŠ” ë°˜ë“œì‹œ ì›ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ì—­ ê¸ˆì§€)
2. ë“±ì¥ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
3. ê° ì—”í‹°í‹°ì— ì˜ë¯¸ ìˆëŠ” attributes ì¶”ê°€
4. ì¤‘ë³µ ì—”í‹°í‹°ëŠ” ì²« ë“±ì¥ë§Œ ì¶”ì¶œ

### ì˜ˆì‹œ
{examples_text}

### ì¶œë ¥ í˜•ì‹ (JSON)
{{
  "extractions": [
    {{
      "extraction_class": "í´ë˜ìŠ¤ëª…",
      "extraction_text": "ì›ë¬¸ í…ìŠ¤íŠ¸",
      "attributes": {{"ì†ì„±í‚¤": "ì†ì„±ê°’"}},
      "description": "ê°„ê²°í•œ ì„¤ëª…"
    }}
  ]
}}
"""


class StructuredEntityExtractor:
    """
    LangExtract-inspired êµ¬ì¡°í™”ëœ ì—”í‹°í‹° ì¶”ì¶œê¸°

    Google LangExtractì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ë¥¼ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„:
    - Few-Shot ê¸°ë°˜ ì¶”ì¶œ (ì‚¬ìš©ì ì •ì˜ ì˜ˆì‹œë¡œ ëª¨ë¸ ê°€ì´ë“œ)
    - ë¬¸ì„œ ì²­í‚¹ + ë³‘ë ¬ ì²˜ë¦¬
    - Multi-Pass Extraction (ë‹¤ì¤‘ íŒ¨ìŠ¤ë¡œ Recall í–¥ìƒ)
    - Source Grounding (ì›ë¬¸ ìœ„ì¹˜ ì¶”ì )
    - í•œêµ­ì–´ Unicode í† í¬ë‚˜ì´ì €

    ì°¸ì¡°: https://github.com/google/langextract

    ì‚¬ìš© ì˜ˆì‹œ:
        extractor = StructuredEntityExtractor(
            prompt_description="ë¬¸ì„œì—ì„œ ì¸ë¬¼, ì¡°ì§, ì‚¬ê±´ì„ ì¶”ì¶œí•˜ì„¸ìš”.",
            examples=[...],
        )
        result = extractor.extract("ë¶„ì„í•  í…ìŠ¤íŠ¸...")
    """

    def __init__(
        self,
        prompt_description: str = "ë¬¸ì„œì—ì„œ ì¸ë¬¼, ì¡°ì§, ì¥ì†Œ, ë‚ ì§œ, ì‚¬ê±´, ì •ì±…, ê¸ˆì•¡ ë“± ì£¼ìš” ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.",
        examples: Optional[List[ExampleData]] = None,
        model_key: str = "gpt-5.2",
        max_chunk_chars: int = 3000,
        extraction_passes: int = 1,
        max_workers: int = 4,
    ):
        self.prompt_description = prompt_description
        self.examples = examples or DEFAULT_KOREAN_EXAMPLES
        self.model_key = model_key
        self.max_chunk_chars = max_chunk_chars
        self.extraction_passes = extraction_passes
        self.max_workers = max_workers

        self.client = AzureClientFactory.get_openai_client(is_advanced=True)
        self.model_name = Config.MODELS.get(model_key, "model-router")
        self._is_gpt5 = "gpt-5" in model_key.lower()
        self.tokenizer = KoreanUnicodeTokenizer()

        # Few-Shot ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì‚¬ì „ êµ¬ì„± (íŒ¨ìŠ¤ë§ˆë‹¤ ì¬ê³„ì‚° ë°©ì§€)
        examples_text = self._format_examples()
        self._system_prompt = EXTRACTION_SYSTEM_PROMPT.format(
            prompt_description=self.prompt_description,
            examples_text=examples_text,
        )

        print(f"ğŸ“‹ StructuredEntityExtractor ì´ˆê¸°í™” "
              f"(ëª¨ë¸: {model_key}, íŒ¨ìŠ¤: {extraction_passes}, ì²­í¬: {max_chunk_chars}ì)")

    def extract(
        self,
        text: str,
        additional_context: str = "",
    ) -> ExtractionResult:
        """
        í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            text: ì¶”ì¶œ ëŒ€ìƒ í…ìŠ¤íŠ¸
            additional_context: ì¶”ê°€ ë¬¸ë§¥ ì •ë³´

        Returns:
            ExtractionResult: ì¶”ì¶œ ê²°ê³¼
        """
        start_time = time.time()

        # 1. í…ìŠ¤íŠ¸ ì²­í‚¹
        chunks = self._chunk_text(text)
        print(f"   ğŸ“„ í…ìŠ¤íŠ¸ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")

        # 2. Multi-Pass Extraction
        all_extractions = []
        for pass_num in range(1, self.extraction_passes + 1):
            if self.extraction_passes > 1:
                print(f"   ğŸ”„ ì¶”ì¶œ íŒ¨ìŠ¤ {pass_num}/{self.extraction_passes}")

            pass_extractions = self._extract_from_chunks(
                chunks, additional_context, pass_num
            )
            all_extractions.append(pass_extractions)

        # 3. íŒ¨ìŠ¤ê°„ ì¤‘ë³µ ì œê±° ë° ë³‘í•©
        merged = self._merge_extractions(all_extractions)

        # 4. Source Grounding (ì›ë¬¸ ìœ„ì¹˜ ì¶”ì )
        self._ground_extractions(text, merged)

        elapsed = time.time() - start_time

        print(f"   âœ… ì¶”ì¶œ ì™„ë£Œ: {len(merged)}ê°œ ì—”í‹°í‹° ({elapsed:.1f}ì´ˆ)")

        return ExtractionResult(
            text=text,
            extractions=merged,
            processing_time=elapsed,
            num_chunks=len(chunks),
            num_passes=self.extraction_passes,
        )

    def extract_from_document_chunks(
        self,
        chunks: List[Any],
        additional_context: str = "",
    ) -> ExtractionResult:
        """
        ì´ë¯¸ ì²­í‚¹ëœ ë¬¸ì„œ ì²­í¬ì—ì„œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.

        Args:
            chunks: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ (page_content ì†ì„± í•„ìš”)
            additional_context: ì¶”ê°€ ë¬¸ë§¥

        Returns:
            ExtractionResult
        """
        start_time = time.time()

        # Document ê°ì²´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text_chunks = []
        for chunk in chunks:
            if hasattr(chunk, "page_content"):
                text_chunks.append(chunk.page_content)
            elif isinstance(chunk, dict):
                text_chunks.append(chunk.get("page_content", str(chunk)))
            else:
                text_chunks.append(str(chunk))

        full_text = "\n\n".join(text_chunks)

        all_extractions = []
        for pass_num in range(1, self.extraction_passes + 1):
            pass_extractions = self._extract_from_chunks(
                text_chunks, additional_context, pass_num
            )
            all_extractions.append(pass_extractions)

        merged = self._merge_extractions(all_extractions)
        self._ground_extractions(full_text, merged)

        elapsed = time.time() - start_time

        return ExtractionResult(
            text=full_text,
            extractions=merged,
            processing_time=elapsed,
            num_chunks=len(text_chunks),
            num_passes=self.extraction_passes,
        )

    def _chunk_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (LangExtractì˜ ChunkIterator ì°¸ì¡°)

        í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„ë¥¼ ì¡´ì¤‘í•˜ì—¬ ë¶„í• í•©ë‹ˆë‹¤.
        """
        if len(text) <= self.max_chunk_chars:
            return [text]

        chunks = []
        # ë‹¨ë½ ë‹¨ìœ„ë¡œ ë¨¼ì € ë¶„í• 
        paragraphs = re.split(r'\n\s*\n', text)
        current_chunk = ""

        for para in paragraphs:
            if len(current_chunk) + len(para) + 2 <= self.max_chunk_chars:
                current_chunk += ("\n\n" + para if current_chunk else para)
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                # ë‹¨ë½ ìì²´ê°€ ë„ˆë¬´ ê¸¸ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                if len(para) > self.max_chunk_chars:
                    sentences = re.split(r'(?<=[.!?ã€‚ï¼Ÿï¼])\s+', para)
                    current_chunk = ""
                    for sent in sentences:
                        if len(current_chunk) + len(sent) + 1 <= self.max_chunk_chars:
                            current_chunk += (" " + sent if current_chunk else sent)
                        else:
                            if current_chunk:
                                chunks.append(current_chunk)
                            current_chunk = sent
                else:
                    current_chunk = para

        if current_chunk:
            chunks.append(current_chunk)

        return chunks

    def _extract_from_chunks(
        self,
        chunks: List[str],
        additional_context: str,
        pass_num: int,
    ) -> List[Extraction]:
        """ì²­í¬ë“¤ì—ì„œ ë³‘ë ¬ë¡œ ì—”í‹°í‹° ì¶”ì¶œ"""
        all_extractions = []

        # ë³‘ë ¬ ì²˜ë¦¬
        if len(chunks) > 1 and self.max_workers > 1:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(
                        self._extract_single_chunk,
                        chunk,
                        self._system_prompt,
                        additional_context,
                    ): idx
                    for idx, chunk in enumerate(chunks)
                }
                for future in as_completed(futures):
                    try:
                        extractions = future.result()
                        all_extractions.extend(extractions)
                    except Exception as e:
                        print(f"      âš ï¸ ì²­í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        else:
            for chunk in chunks:
                extractions = self._extract_single_chunk(
                    chunk, self._system_prompt, additional_context
                )
                all_extractions.extend(extractions)

        return all_extractions

    def _extract_single_chunk(
        self,
        chunk_text: str,
        system_prompt: str,
        additional_context: str,
    ) -> List[Extraction]:
        """ë‹¨ì¼ ì²­í¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ"""
        user_message = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:\n\n{chunk_text}"
        if additional_context:
            user_message += f"\n\nì¶”ê°€ ë¬¸ë§¥: {additional_context}"

        try:
            completion_params = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                "temperature": 0.0,
                "response_format": {"type": "json_object"},
            }

            if self._is_gpt5:
                completion_params["max_completion_tokens"] = 4000
            else:
                completion_params["max_tokens"] = 4000

            response = self.client.chat.completions.create(**completion_params)
            result_text = response.choices[0].message.content
            result = json.loads(result_text)

            extractions = []
            for ext_data in result.get("extractions", []):
                extraction = Extraction(
                    extraction_class=ext_data.get("extraction_class", "ê¸°íƒ€"),
                    extraction_text=ext_data.get("extraction_text", ""),
                    attributes=ext_data.get("attributes", {}),
                    description=ext_data.get("description", ""),
                )
                if extraction.extraction_text:  # ë¹ˆ í…ìŠ¤íŠ¸ ì œì™¸
                    extractions.append(extraction)

            return extractions

        except Exception as e:
            print(f"      âš ï¸ LLM ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []

    def _format_examples(self) -> str:
        """Few-Shot ì˜ˆì‹œë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        parts = []
        for i, example in enumerate(self.examples, 1):
            parts.append(f"ì˜ˆì‹œ {i}:")
            parts.append(f"í…ìŠ¤íŠ¸: {example.text}")
            for ext in example.extractions:
                attrs = ", ".join(f'{k}: {v}' for k, v in ext.attributes.items())
                parts.append(
                    f"  â†’ [{ext.extraction_class}] \"{ext.extraction_text}\""
                    + (f" ({attrs})" if attrs else "")
                )
            parts.append("")
        return "\n".join(parts)

    def _merge_extractions(
        self,
        all_passes: List[List[Extraction]],
    ) -> List[Extraction]:
        """
        Multi-Pass ì¶”ì¶œ ê²°ê³¼ ë³‘í•© (LangExtractì˜ _merge_non_overlapping_extractions ì°¸ì¡°)

        ì¤‘ë³µ ì—”í‹°í‹°ë¥¼ ì œê±°í•˜ê³ , ê°€ì¥ ì™„ì „í•œ ê²°ê³¼ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
        """
        if len(all_passes) == 1:
            return self._deduplicate(all_passes[0])

        # ëª¨ë“  íŒ¨ìŠ¤ì˜ ê²°ê³¼ë¥¼ í•©ì¹¨
        combined = []
        for pass_extractions in all_passes:
            combined.extend(pass_extractions)

        return self._deduplicate(combined)

    def _deduplicate(self, extractions: List[Extraction]) -> List[Extraction]:
        """ì¶”ì¶œ ê²°ê³¼ ì¤‘ë³µ ì œê±°"""
        seen = set()
        unique = []

        for ext in extractions:
            # (í´ë˜ìŠ¤, í…ìŠ¤íŠ¸) ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬
            key = (ext.extraction_class, ext.extraction_text.strip())
            if key not in seen:
                seen.add(key)
                unique.append(ext)

        return unique

    def _ground_extractions(
        self,
        full_text: str,
        extractions: List[Extraction],
    ) -> None:
        """
        Source Grounding: ì¶”ì¶œëœ ì—”í‹°í‹°ì˜ ì›ë¬¸ ìœ„ì¹˜ë¥¼ ì¶”ì í•©ë‹ˆë‹¤.
        (LangExtractì˜ Resolver.align ì°¸ì¡°)
        """
        for ext in extractions:
            positions = self.tokenizer.find_text_positions(
                full_text, ext.extraction_text, fuzzy=True
            )
            if positions:
                ext.char_interval = positions[0]  # ì²« ë“±ì¥ ìœ„ì¹˜
                ext.alignment_status = "aligned"
            else:
                ext.alignment_status = "unaligned"

    def extractions_to_dict(
        self,
        result: ExtractionResult,
    ) -> List[Dict[str, Any]]:
        """ì¶”ì¶œ ê²°ê³¼ë¥¼ ì§ë ¬í™” ê°€ëŠ¥í•œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return [
            {
                "extraction_class": e.extraction_class,
                "extraction_text": e.extraction_text,
                "char_interval": {
                    "start": e.char_interval.start_pos,
                    "end": e.char_interval.end_pos,
                } if e.char_interval else None,
                "attributes": e.attributes,
                "description": e.description,
                "alignment_status": e.alignment_status,
            }
            for e in result.extractions
        ]
