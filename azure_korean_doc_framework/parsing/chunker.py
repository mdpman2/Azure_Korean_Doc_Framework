import re
import tiktoken
from enum import Enum
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field

from ..core.schema import Document
from ..core.multi_model_manager import MultiModelManager

from ..config import Config


@dataclass
class ChunkingConfig:
    """ì²­í‚¹ ì„¤ì •ì„ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    min_tokens: int = 100          # ìµœì†Œ í† í° ìˆ˜
    max_tokens: int = 500          # ìµœëŒ€ í† í° ìˆ˜
    target_tokens: int = 300       # ëª©í‘œ í† í° ìˆ˜
    overlap_tokens: int = 50       # ì˜¤ë²„ë© í† í° ìˆ˜ (ì•½ 10-15%)
    encoding_name: str = "cl100k_base"  # tiktoken ì¸ì½”ë”© (GPT-4, text-embedding-ada-002ìš©)


class ChunkingStrategy(Enum):
    LEGAL = "legal"
    HIERARCHICAL = "hierarchical"
    TABULAR = "tabular"
    FALLBACK = "fallback"


class AdaptiveChunker:
    """
    ë¬¸ì„œì˜ íŠ¹ì„±(íŒŒì¼ëª…, ë‚´ìš© êµ¬ì¡°)ì— ë”°ë¼ ìµœì ì˜ ì²­í‚¹ ì „ëµì„ ë™ì ìœ¼ë¡œ ì„ íƒí•˜ëŠ” Chunker.

    ê°œì„ ëœ ê¸°ëŠ¥:
    - í† í° ê¸°ë°˜ ì²­í¬ í¬ê¸° ì œì–´
    - ì²­í¬ ê°„ ì˜¤ë²„ë©ìœ¼ë¡œ ë¬¸ë§¥ ì—°ì†ì„± ë³´ì¥
    - í•œêµ­ì–´ ë¬¸ì¥ ê²½ê³„ ì¸ì‹
    - ê°•í™”ëœ ë©”íƒ€ë°ì´í„°
    """

    def __init__(self, config: Optional[ChunkingConfig] = None):
        self.config = config or ChunkingConfig()

        # tiktoken ì¸ì½”ë” ì´ˆê¸°í™”
        self.encoder = tiktoken.get_encoding(self.config.encoding_name)

        # MultiModelManager ì´ˆê¸°í™” (Contextual Retrievalìš©)
        self.model_manager = MultiModelManager()

    # ==================== í† í° ê´€ë ¨ ìœ í‹¸ë¦¬í‹° ====================

    def _count_tokens(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        return len(self.encoder.encode(text))

    def _split_korean_sentences(self, text: str) -> List[str]:
        """
        í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•©ë‹ˆë‹¤.
        kss ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì‹¤íŒ¨ ì‹œ ì •ê·œì‹ ê¸°ë°˜ ë¶„ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        """
        try:
            import kss
            sentences = kss.split_sentences(text)
            return sentences
        except Exception:
            # Fallback: ì •ê·œì‹ ê¸°ë°˜ í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬
            # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ë’¤ì— ê³µë°±ì´ë‚˜ ì¤„ë°”ê¿ˆì´ ì˜¤ëŠ” ê²½ìš° ë¶„ë¦¬
            pattern = r'(?<=[.!?ã€‚ï¼Ÿï¼])\s+'
            sentences = re.split(pattern, text)
            return [s.strip() for s in sentences if s.strip()]

    def _merge_sentences_to_chunks(
        self,
        sentences: List[str],
        overlap_sentences: int = 1
    ) -> List[str]:
        """
        ë¬¸ì¥ë“¤ì„ í† í° ì œí•œì— ë§ê²Œ ì²­í¬ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.
        ì˜¤ë²„ë©ì„ ì ìš©í•˜ì—¬ ë¬¸ë§¥ ì—°ì†ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.
        """
        if not sentences:
            return []

        chunks = []
        current_chunk_sentences = []
        current_token_count = 0

        for sentence in sentences:
            sentence_tokens = self._count_tokens(sentence)

            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
            if current_token_count + sentence_tokens <= self.config.max_tokens:
                current_chunk_sentences.append(sentence)
                current_token_count += sentence_tokens
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk_sentences:
                    chunks.append(" ".join(current_chunk_sentences))

                # ì˜¤ë²„ë© ì ìš©: ë§ˆì§€ë§‰ Nê°œ ë¬¸ì¥ì„ ë‹¤ìŒ ì²­í¬ì— í¬í•¨
                overlap_start = max(0, len(current_chunk_sentences) - overlap_sentences)
                overlap_sents = current_chunk_sentences[overlap_start:]

                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk_sentences = overlap_sents + [sentence]
                current_token_count = sum(self._count_tokens(s) for s in current_chunk_sentences)

        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk_sentences:
            chunks.append(" ".join(current_chunk_sentences))

        return chunks

    def _split_with_overlap(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ í›„ ì˜¤ë²„ë©ì„ ì ìš©í•˜ì—¬ ì²­í‚¹í•©ë‹ˆë‹¤.
        """
        if not text or not text.strip():
            return []

        # 1. í•œêµ­ì–´ ë¬¸ì¥ ë¶„ë¦¬
        sentences = self._split_korean_sentences(text)

        if not sentences:
            return [text] if self._count_tokens(text) <= self.config.max_tokens else []

        # 2. ì˜¤ë²„ë© ì ìš©í•˜ì—¬ ì²­í¬ ìƒì„±
        return self._merge_sentences_to_chunks(sentences, overlap_sentences=2)

    def _enrich_metadata(
        self,
        base_metadata: Dict[str, Any],
        chunk_index: int,
        total_chunks: int,
        chunk_text: str,
        section_title: str = ""
    ) -> Dict[str, Any]:
        """ì²­í¬ì— ê°•í™”ëœ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        enriched = base_metadata.copy()
        enriched.update({
            "chunk_index": chunk_index,
            "total_chunks": total_chunks,
            "token_count": self._count_tokens(chunk_text),
            "char_count": len(chunk_text),
            "section_title": section_title,
        })
        return enriched

    # ==================== ë©”ì¸ ì²­í‚¹ ë¡œì§ ====================

    def chunk(self, segments: List[Dict[str, Any]], filename: str = "", extra_metadata: Optional[Dict[str, Any]] = None) -> List[Document]:
        """
        Main Entrypoint: ë¬¸ì„œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ì ì ˆí•œ ì „ëµìœ¼ë¡œ ì²­í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if extra_metadata is None: extra_metadata = {}

        # 1. ë¬¸ì„œ ë¶„ë¥˜
        strategy = self._classify_document(filename, segments)
        print(f"ğŸ” Document Classification: '{filename}' -> {strategy.name}")
        print(f"   âš™ï¸ Config: min={self.config.min_tokens}, max={self.config.max_tokens}, overlap={self.config.overlap_tokens} tokens")

        # 2. ì „ëµ ì‹¤í–‰ (Dispatcher)
        if strategy == ChunkingStrategy.LEGAL:
            chunks = self._chunk_legal(segments, extra_metadata, filename=filename)
        elif strategy == ChunkingStrategy.TABULAR:
            chunks = self._chunk_tabular(segments, extra_metadata)
        elif strategy == ChunkingStrategy.HIERARCHICAL:
            chunks = self._chunk_hierarchical(segments, extra_metadata)
        else:
            chunks = self._chunk_fallback(segments, extra_metadata)

        # 3. ìµœì¢… ë©”íƒ€ë°ì´í„° ê°•í™”
        total = len(chunks)
        for i, chunk in enumerate(chunks):
            chunk.metadata = self._enrich_metadata(
                chunk.metadata,
                chunk_index=i,
                total_chunks=total,
                chunk_text=chunk.page_content,
                section_title=chunk.metadata.get("breadcrumb", "")
            )

        print(f"   âœ… Generated {total} chunks")
        return chunks

    def _classify_document(self, filename: str, segments: List[Dict[str, Any]]) -> ChunkingStrategy:
        """íŒŒì¼ëª…ê³¼ ì½˜í…ì¸  ë¹„ìœ¨ì„ ê¸°ë°˜ìœ¼ë¡œ ì²­í‚¹ ì „ëµì„ ê²°ì •í•©ë‹ˆë‹¤."""
        name = filename.lower()

        # 1. Legal Strategy (íŒŒì¼ëª… or íŠ¹ì • í‚¤ì›Œë“œ)
        if any(k in name for k in ["[ë¯¼ì‚¬]", "[í˜•ì‚¬]", "[í–‰ì •]", "[íŠ¹í—ˆ]", "íŒë¡€"]):
            return ChunkingStrategy.LEGAL

        # 2. Tabular Strategy (íŒŒì¼ëª… or í‘œ ë¹„ì¤‘)
        if any(k in name for k in ["ì¬ì •ë™í–¥", "í†µí™”ì‹ ìš©ì •ì±…", "í˜„í™©"]):
            return ChunkingStrategy.TABULAR

        # í‘œê°€ ì „ì²´ ì„¸ê·¸ë¨¼íŠ¸ì˜ 50% ì´ìƒì´ë©´ Tabularë¡œ ê°„ì£¼
        table_count = sum(1 for s in segments if s['type'] == 'table')
        if len(segments) > 0 and (table_count / len(segments)) > 0.5:
            return ChunkingStrategy.TABULAR

        # 3. Hierarchical Strategy (ê¸°ë³¸ ë³´ê³ ì„œ)
        header_count = sum(1 for s in segments if s['type'] == 'header')
        if header_count > 0:
            return ChunkingStrategy.HIERARCHICAL

        # 4. Fallback
        return ChunkingStrategy.FALLBACK

    def _generate_context(self, chunk_text: str, filename: str) -> str:
        """Contextual Retrieval: ì²­í¬ì˜ ë¬¸ë§¥ì„ LLMì„ í†µí•´ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            system_prompt = (
                "You are a legal expert assistant.\n"
                f"The following is a section from a document named '{filename}'.\n"
                "Please read the section and explain its specific context in 2-3 sentences.\n"
                "Focus on identifying the legal principle, the court's reasoning, or the specific subject matter being discussed.\n"
                "Do not just repeat the text, but contextualize it so it can be understood in isolation."
            )

            user_prompt = f"Section Content:\n{chunk_text}\n\nContext Explanation:"

            context = self.model_manager.get_completion(
                prompt=user_prompt,
                system_message=system_prompt,
                model_key="gpt-4o", # ê¸°ë³¸ì ìœ¼ë¡œ ê³ ì„±ëŠ¥ ëª¨ë¸ ì‚¬ìš©
                temperature=0
            )
            return context.strip()
        except Exception as e:
            print(f"âš ï¸ Context Generation Failed: {e}")
            return ""

    def _chunk_legal(self, segments: List[Dict[str, Any]], extra_metadata: Dict[str, Any], filename: str) -> List[Document]:
        """Strategy A: Regex-based split for Legal documents + Contextual Retrieval"""
        print("   âš–ï¸ Strategy: LEGAL (Regex Split + Contextual Retrieval + Overlap)")

        # 1. 1ì°¨ í†µí•©
        full_text = "\n\n".join([s['content'] for s in segments])

        # 2. Regex Split (íŒë¡€ êµ¬ì¡° ê¸°ë°˜ - ã€ì£¼ë¬¸ã€‘, ã€ì´ìœ ã€‘ ë“±)
        split_pattern = r"(?=ã€.*?ã€‘)"
        raw_chunks = re.split(split_pattern, full_text)

        final_chunks = []
        print(f"      ğŸ‘‰ Splitting into {len(raw_chunks)} raw blocks...")

        for i, raw_text in enumerate(raw_chunks):
            if not raw_text.strip(): continue

            # í† í° ìˆ˜ ì²´í¬ - ë„ˆë¬´ í¬ë©´ ì¶”ê°€ ë¶„í• 
            if self._count_tokens(raw_text) > self.config.max_tokens:
                sub_chunks = self._split_with_overlap(raw_text)
                for j, sub_chunk in enumerate(sub_chunks):
                    meta = extra_metadata.copy()
                    meta['strategy'] = 'legal_contextual'
                    meta['sub_chunk'] = f"{i+1}.{j+1}"
                    final_chunks.append(Document(page_content=sub_chunk, metadata=meta))
            else:
                # 3. Contextual Retrieval (LLM í˜¸ì¶œ) - ë¹„ìš© ì ˆê°ì„ ìœ„í•´ ì²« 5ê°œë§Œ
                context = ""
                if i < 5:
                    print(f"      ğŸ¤– Generating context for chunk {i+1}...")
                    context = self._generate_context(raw_text[:1000], filename)

                content_with_context = raw_text
                if context:
                    content_with_context = f"[Context: {context}]\n\n{raw_text}"

                meta = extra_metadata.copy()
                meta['strategy'] = 'legal_contextual'
                final_chunks.append(Document(page_content=content_with_context, metadata=meta))

        return final_chunks

    def _chunk_tabular(self, segments: List[Dict[str, Any]], extra_metadata: Dict[str, Any]) -> List[Document]:
        """Strategy C: Row-wise serialization for Data/Table heavy documents"""
        print("   ğŸ“Š Strategy: TABULAR (Row-wise Serialization + Token Control)")
        final_chunks = []

        for seg in segments:
            if seg['type'] == 'table':
                # ë§ˆí¬ë‹¤ìš´ í‘œ -> ìì—°ì–´ ë¬¸ì¥ ë³€í™˜
                sentences = self._markdown_table_to_sentences(seg['content'])

                serialized_text = "\n".join(sentences)
                if not serialized_text:
                    serialized_text = seg['content']  # ì‹¤íŒ¨ ì‹œ ì›ë¬¸

                # í† í° ìˆ˜ ì²´í¬ - ë„ˆë¬´ í¬ë©´ ë¶„í• 
                if self._count_tokens(serialized_text) > self.config.max_tokens:
                    sub_chunks = self._split_with_overlap(serialized_text)
                    for sub_chunk in sub_chunks:
                        meta = extra_metadata.copy()
                        meta['is_table_data'] = True
                        meta["page"] = seg.get("page", 1)
                        final_chunks.append(Document(page_content=sub_chunk, metadata=meta))
                else:
                    meta = extra_metadata.copy()
                    meta['is_table_data'] = True
                    meta["page"] = seg.get("page", 1)
                    final_chunks.append(Document(page_content=serialized_text, metadata=meta))

            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ì˜¤ë²„ë© ì²­í‚¹
                text_content = seg['content'].strip()
                if text_content and len(text_content) > 10:
                    sub_chunks = self._split_with_overlap(text_content)
                    for sub_chunk in sub_chunks:
                        if self._count_tokens(sub_chunk) >= self.config.min_tokens:
                            final_chunks.append(Document(page_content=sub_chunk, metadata=extra_metadata.copy()))

        return final_chunks

    def _markdown_table_to_sentences(self, markdown_table: str) -> List[str]:
        """Markdown í…Œì´ë¸”ì„ 'í—¤ë”ëŠ” ê°’ì´ë‹¤' í˜•íƒœì˜ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        lines = markdown_table.strip().split('\n')
        if len(lines) < 3: return []

        header_line = lines[0]
        data_lines = lines[2:]  # êµ¬ë¶„ì„  ê±´ë„ˆê¹€

        headers = [h.strip() for h in header_line.split('|') if h.strip()]
        sentences = []

        for row in data_lines:
            cells = [c.strip() for c in row.split('|') if c.strip()]
            if not cells: continue

            row_parts = []
            for h, c in zip(headers, cells):
                if h and c:
                    row_parts.append(f"{h}ì€(ëŠ”) {c}")

            if row_parts:
                sentences.append(", ".join(row_parts) + ".")

        return sentences

    def _chunk_hierarchical(self, segments: List[Dict[str, Any]], extra_metadata: Dict[str, Any]) -> List[Document]:
        """Strategy B: Context-Rich Rolling Window with Overlap"""
        print("   ğŸŒ² Strategy: HIERARCHICAL (Context-Rich + Overlap)")
        final_chunks = []
        header_stack = []  # [(level, text), ...]
        text_buffer = []

        def get_breadcrumb():
            return " > ".join([h[1] for h in header_stack])

        def flush_text_buffer():
            if not text_buffer: return
            combined_text = "\n\n".join(text_buffer)
            text_buffer.clear()

            current_breadcrumb = get_breadcrumb()

            if not combined_text or len(combined_text) < 10:
                return

            # ì˜¤ë²„ë© ì ìš© ì²­í‚¹
            sub_chunks = self._split_with_overlap(combined_text)

            for sub_chunk in sub_chunks:
                if self._count_tokens(sub_chunk) < self.config.min_tokens:
                    continue

                base_meta = extra_metadata.copy()
                base_meta["breadcrumb"] = current_breadcrumb
                base_meta["type"] = "text"

                content = f"[{current_breadcrumb}]\n{sub_chunk}" if current_breadcrumb else sub_chunk
                final_chunks.append(Document(page_content=content, metadata=base_meta))

        for seg in segments:
            seg_type = seg["type"]
            content = seg["content"]

            if seg_type == "header":
                flush_text_buffer()
                level = 0
                clean_header = content.strip()
                if clean_header.startswith("#"):
                    level = len(clean_header.split()[0])
                    clean_header = clean_header.lstrip("#").strip()
                else:
                    level = 1

                while header_stack and header_stack[-1][0] >= level:
                    header_stack.pop()
                header_stack.append((level, clean_header))

            elif seg_type == "table":
                flush_text_buffer()
                current_breadcrumb = get_breadcrumb()

                # í‘œ ì§ë ¬í™”
                sentences = self._markdown_table_to_sentences(content)
                serialized = "\n".join(sentences) if sentences else content

                full_content = f"[{current_breadcrumb}]\n{serialized}" if current_breadcrumb else serialized

                meta = extra_metadata.copy()
                meta["breadcrumb"] = current_breadcrumb
                meta["type"] = "table"
                meta["page"] = seg.get("page", 1)
                final_chunks.append(Document(page_content=full_content, metadata=meta))

            elif seg_type in ["text", "image"]:
                if content.strip():
                    text_buffer.append(content)

        flush_text_buffer()
        return final_chunks

    def _chunk_fallback(self, segments: List[Dict[str, Any]], extra_metadata: Dict[str, Any]) -> List[Document]:
        """Strategy D: Simple Fallback with Overlap"""
        print("   ğŸ‚ Strategy: FALLBACK (Overlap Chunking)")
        all_text = "\n\n".join([s['content'] for s in segments if s.get('content', '').strip()])

        if not all_text or len(all_text.strip()) < 10:
            print("   âš ï¸ No content to chunk")
            return []

        # ì˜¤ë²„ë© ì ìš© ì²­í‚¹
        sub_chunks = self._split_with_overlap(all_text)

        final_chunks = []
        for sub_chunk in sub_chunks:
            if self._count_tokens(sub_chunk) >= self.config.min_tokens:
                final_chunks.append(Document(page_content=sub_chunk, metadata=extra_metadata.copy()))

        return final_chunks


# Backward Compatibility
KoreanSemanticChunker = AdaptiveChunker
