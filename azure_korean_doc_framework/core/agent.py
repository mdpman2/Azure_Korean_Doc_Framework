from typing import List, Tuple, Optional, Union, Any, Dict
from azure.search.documents.models import VectorizedQuery
from .multi_model_manager import MultiModelManager
from ..utils.azure_clients import AzureClientFactory
from ..config import Config

class KoreanDocAgent:
    """
    í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ ë° Q&A ì „ë¬¸ê°€ ê²€ìƒ‰ ì—ì´ì „íŠ¸.

    Azure AI Searchì˜ Hybrid Search + Semantic Rankingì„ í™œìš©í•˜ì—¬ ë¬¸ë§¥ì„ ì°¾ê³ ,
    GPT-5.2ë¥¼ í†µí•´ ì§€ëŠ¥ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    [2026-01 ì—…ë°ì´íŠ¸]
    - GPT-5.2 ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
    - Query Rewrite ì§€ì› (ì‹œë§¨í‹± ì¿¼ë¦¬ í™•ì¥)
    - í–¥ìƒëœ Semantic Ranking (L2 reranking)
    - Agentic Retrieval íŒ¨í„´ ì§€ì› ì¤€ë¹„
    """

    def __init__(self, model_key: Optional[str] = None):
        """
        KoreanDocAgentë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            model_key: ë‹µë³€ ìƒì„± ì‹œ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•  ëª¨ë¸ í‚¤ (Config.MODELSì— ì •ì˜ëœ í‚¤).
                      ê¸°ë³¸ê°’: Config.DEFAULT_MODEL (gpt-5.2)
        """
        self.model_manager = MultiModelManager(default_model=model_key or Config.DEFAULT_MODEL)
        self.search_client = AzureClientFactory.get_search_client()

        # ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (ë²¡í„° ê²€ìƒ‰ìš©) - ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (text-embedding-3-small)
        self.embedding_client = AzureClientFactory.get_openai_client(is_advanced=False)

        # LLM í´ë¼ì´ì–¸íŠ¸ (Query Rewriteìš©) - ê³ ì„±ëŠ¥ ì—”ë“œí¬ì¸íŠ¸
        self.llm_client = AzureClientFactory.get_openai_client(is_advanced=True)

        # Query Rewrite í™œì„±í™” ì—¬ë¶€
        self.enable_query_rewrite = True

    def _rewrite_query(self, question: str) -> List[str]:
        """
        GPT-5.2ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ í™•ì¥í•©ë‹ˆë‹¤.
        ì˜¤íƒ€ êµì •, ë™ì˜ì–´ ìƒì„±, ë‹¤ì–‘í•œ í‘œí˜„ìœ¼ë¡œ ì¿¼ë¦¬ ë³€í˜•.

        Args:
            question: ì›ë³¸ ì§ˆë¬¸

        Returns:
            í™•ì¥ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ í¬í•¨)
        """
        if not self.enable_query_rewrite:
            return [question]

        try:
            rewrite_prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ ì—¬ëŸ¬ í˜•íƒœë¡œ ë³€í˜•í•´ì£¼ì„¸ìš”.
ì˜¤íƒ€ êµì •, ë™ì˜ì–´ ì‚¬ìš©, ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ì„ í¬í•¨í•˜ì„¸ìš”.
ì›ë³¸ ì§ˆë¬¸ë„ í¬í•¨í•˜ì—¬ ìµœëŒ€ 3ê°œì˜ ì¿¼ë¦¬ë¥¼ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ì¶œë ¥ í˜•ì‹: ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]"""

            response = self.llm_client.chat.completions.create(
                model=Config.MODELS.get("gpt-5.2", "gpt-5.2"),
                messages=[{"role": "user", "content": rewrite_prompt}],
                temperature=0.3,
                max_completion_tokens=200
            )

            import json
            result = response.choices[0].message.content.strip()
            # JSON ë°°ì—´ íŒŒì‹±
            if result.startswith("["):
                queries = json.loads(result)
                return queries[:3] if queries else [question]
            return [question]

        except Exception as e:
            print(f"   âš ï¸ Query rewrite failed, using original: {e}")
            return [question]

    def answer_question(
        self,
        question: str,
        model_key: Optional[str] = None,
        return_context: bool = False,
        top_k: int = 5,
        use_query_rewrite: bool = True
    ) -> Union[str, Tuple[str, List[str]]]:
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        1. Query Rewrite (ì„ íƒì ): ì§ˆë¬¸ì„ ì˜ë¯¸ì ìœ¼ë¡œ í™•ì¥
        2. AI Searchì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„°+í‚¤ì›Œë“œ) ë° ì‹œë§¨í‹± ë­í‚¹ ìˆ˜í–‰
        3. ê²€ìƒ‰ëœ ë¬¸ë§¥(Context)ì„ ë°”íƒ•ìœ¼ë¡œ GPT-5.2ë¡œ ë‹µë³€ ìƒì„± (ì¶œì²˜ ì •ë³´ í¬í•¨)

        Args:
            question: ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë¬¸ìì—´.
            model_key: ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  íŠ¹ì • ëª¨ë¸ í‚¤.
            return_context: Trueì¼ ê²½ìš° ë‹µë³€ê³¼ í•¨ê»˜ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œì˜ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5).
            use_query_rewrite: Query Rewrite ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True).

        Returns:
            ë‹µë³€ ë¬¸ìì—´ ë˜ëŠ” (ë‹µë³€, ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸) íŠœí”Œ.
        """
        print(f"ğŸ” Searching for: {question} (top_k={top_k})")

        # 0. Query Rewrite (ì„ íƒì )
        search_queries = [question]
        if use_query_rewrite and self.enable_query_rewrite:
            search_queries = self._rewrite_query(question)
            if len(search_queries) > 1:
                print(f"   ğŸ“ Query expanded to {len(search_queries)} variants")

        # 1. ì§ˆë¬¸ ì„ë² ë”© ìƒì„± (ë²¡í„° ê²€ìƒ‰ìš©) - ì›ë³¸ ì§ˆë¬¸ ì‚¬ìš©
        embedding_response = self.embedding_client.embeddings.create(
            input=[question],
            model=Config.EMBEDDING_DEPLOYMENT
        )
        query_vector = embedding_response.data[0].embedding
        vector_query = VectorizedQuery(vector=query_vector, k_nearest_neighbors=50, fields="text_vector")

        # 2. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë° ì‹œë§¨í‹± ë­í‚¹ ìˆ˜í–‰ (ëª¨ë“  ì¿¼ë¦¬ ë³€í˜•ì— ëŒ€í•´)
        all_contexts = []
        all_sources = set()

        try:
            for search_query in search_queries:
                results = self.search_client.search(
                    search_text=search_query,
                    vector_queries=[vector_query],
                    select=["chunk", "parent_id"],
                    query_type="semantic",
                    semantic_configuration_name="my-semantic-config",
                    top=top_k
                )

                for r in results:
                    content = r.get('chunk') or r.get('content') or ""
                    source = r.get('parent_id') or "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"

                    # ì¤‘ë³µ ì œê±°
                    context_entry = f"[ì¶œì²˜: {source}]\n{content}"
                    if content and context_entry not in all_contexts:
                        all_contexts.append(context_entry)
                        all_sources.add(source)

        except Exception as e:
            print(f"   âŒ Search failed: {e}")
            all_contexts = []

        # ìƒìœ„ top_k * 2ê°œë§Œ ìœ ì§€ (ì¤‘ë³µ ì œê±° í›„)
        contexts = all_contexts[:top_k * 2]
        context_str = "\n\n".join(contexts)

        if not context_str:
            print("   âš ï¸ No relevant documentation found.")
            context_str = "ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        system_prompt = (
            "ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ë° Q&A ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ [Context] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ [Question]ì— í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
            "\n\n### ë‹µë³€ ê·œì¹™:"
            "\n1. ë‹µë³€ ì‹œ ë°˜ë“œì‹œ í•´ë‹¹ ì •ë³´ì˜ **ì¶œì²˜(íŒŒì¼ëª…)**ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”. (ì˜ˆ: '...ì…ë‹ˆë‹¤ [ì¶œì²˜: íŒŒì¼ëª….pdf]')"
            "\n2. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì·¨í•©í•œ ê²½ìš°, ê°ê°ì˜ ì¶œì²˜ë¥¼ ë°íˆì„¸ìš”."
            "\n3. ì¶”ì¶œëœ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì•„ëŠ” ë²”ìœ„ ë‚´ì—ì„œ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•˜ë˜, ì •ë³´ê°€ ì „í˜€ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”."
        )

        user_prompt = f"[Context]\n{context_str}\n\n[Question]\n{question}"

        # LLM í˜¸ì¶œì„ í†µí•œ ë‹µë³€ ìƒì„±
        answer = self.model_manager.get_completion(
            prompt=user_prompt,
            model_key=model_key,
            system_message=system_prompt
        )

        if return_context:
            return answer, contexts
        return answer
