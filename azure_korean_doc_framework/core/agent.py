import json
from typing import List, Tuple, Optional, Union
from azure.search.documents.models import VectorizedQuery
from .multi_model_manager import MultiModelManager
from ..utils.azure_clients import AzureClientFactory
from ..config import Config

# ê³µí†µ RAG ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (answer_question / graph_enhanced_answer ê³µìœ )
_RAG_SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ë° Q&A ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
    "ì£¼ì–´ì§„ [Context] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ [Question]ì— í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”. "
    "\n\n### ë‹µë³€ ê·œì¹™:"
    "\n1. ë‹µë³€ ì‹œ ë°˜ë“œì‹œ í•´ë‹¹ ì •ë³´ì˜ **ì¶œì²˜(íŒŒì¼ëª…)**ë¥¼ ì–¸ê¸‰í•˜ì„¸ìš”. (ì˜ˆ: '...ì…ë‹ˆë‹¤ [ì¶œì²˜: íŒŒì¼ëª….pdf]')"
    "\n2. ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì·¨í•©í•œ ê²½ìš°, ê°ê°ì˜ ì¶œì²˜ë¥¼ ë°íˆì„¸ìš”."
    "\n3. ì¶”ì¶œëœ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì•„ëŠ” ë²”ìœ„ ë‚´ì—ì„œ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•˜ë˜, ì •ë³´ê°€ ì „í˜€ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”."
)

_GRAPH_RAG_SYSTEM_PROMPT = (
    _RAG_SYSTEM_PROMPT
    + "\n4. Knowledge Graph ì •ë³´ê°€ ìˆìœ¼ë©´ ì—”í‹°í‹° ê°„ ê´€ê³„ë¥¼ í™œìš©í•˜ì—¬ ë” í’ë¶€í•œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”."
)


class KoreanDocAgent:
    """
    í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ ë° Q&A ì „ë¬¸ê°€ ê²€ìƒ‰ ì—ì´ì „íŠ¸.

    Azure AI Searchì˜ Hybrid Search + Semantic Rankingì„ í™œìš©í•˜ì—¬ ë¬¸ë§¥ì„ ì°¾ê³ ,
    GPT-5.2ë¥¼ í†µí•´ ì§€ëŠ¥ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

    [2026-02 v4.0 ì—…ë°ì´íŠ¸]
    - Graph-Enhanced RAG (LightRAG ê¸°ë°˜ Knowledge Graph ì—°ë™)
    - êµ¬ì¡°í™” ì—”í‹°í‹° ì¶”ì¶œ ê²°ê³¼ í™œìš© (LangExtract ê¸°ë°˜)
    - GPT-5.2 ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
    - Query Rewrite ì§€ì› (ì‹œë§¨í‹± ì¿¼ë¦¬ í™•ì¥)
    - í–¥ìƒëœ Semantic Ranking (L2 reranking)
    - Dual-Mode ê²€ìƒ‰: Vector + Graph í•˜ì´ë¸Œë¦¬ë“œ
    """

    def __init__(self, model_key: Optional[str] = None, graph_manager=None):
        """
        KoreanDocAgentë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            model_key: ë‹µë³€ ìƒì„± ì‹œ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•  ëª¨ë¸ í‚¤ (Config.MODELSì— ì •ì˜ëœ í‚¤).
                      ê¸°ë³¸ê°’: Config.DEFAULT_MODEL (gpt-5.2)
            graph_manager: KnowledgeGraphManager ì¸ìŠ¤í„´ìŠ¤ (Graph RAG ì‚¬ìš© ì‹œ)
        """
        self.model_manager = MultiModelManager(default_model=model_key or Config.DEFAULT_MODEL)
        self.search_client = AzureClientFactory.get_search_client()

        # ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (ë²¡í„° ê²€ìƒ‰ìš©) - ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš© (text-embedding-3-small)
        self.embedding_client = AzureClientFactory.get_openai_client(is_advanced=False)

        # LLM í´ë¼ì´ì–¸íŠ¸ (Query Rewriteìš©) - ê³ ì„±ëŠ¥ ì—”ë“œí¬ì¸íŠ¸
        self.llm_client = AzureClientFactory.get_openai_client(is_advanced=True)

        # Query Rewrite í™œì„±í™” ì—¬ë¶€
        self.enable_query_rewrite = True

        # [v4.0] Graph RAG ë§¤ë‹ˆì € (LightRAG ê¸°ë°˜)
        self.graph_manager = graph_manager

    def _rewrite_query(self, question: str) -> List[str]:
        """
        GPT-5.2ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ í™•ì¥í•©ë‹ˆë‹¤.
        ì˜¤íƒ€ êµì •, ë™ì˜ì–´ ìƒì„±, ë‹¤ì–‘í•œ í‘œí˜„ìœ¼ë¡œ ì¿¼ë¦¬ ë³€í˜•.

        Args:
            question: ì›ë³¸ ì§ˆë¬¸

        Returns:
            í™•ì¥ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (ì›ë³¸ í¬í•¨)
        """
        if not self.enable_query_rewrite:
            return [question]

        try:
            rewrite_prompt = f"""ë‹¤ìŒ í•œêµ­ì–´ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ ì—¬ëŸ¬ í˜•íƒœë¡œ ë³€í˜•í•´ì£¼ì„¸ìš”.
ì˜¤íƒ€ êµì •, ë™ì˜ì–´ ì‚¬ìš©, ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ì„ í¬í•¨í•˜ì„¸ìš”.
ì›ë³¸ ì§ˆë¬¸ë„ í¬í•¨í•˜ì—¬ ìµœëŒ€ 3ê°œì˜ ì¿¼ë¦¬ë¥¼ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {question}

ì¶œë ¥ í˜•ì‹: ["ì¿¼ë¦¬1", "ì¿¼ë¦¬2", "ì¿¼ë¦¬3"]"""

            response = self.llm_client.chat.completions.create(
                model=Config.MODELS.get("gpt-5.2", "gpt-5.2"),
                messages=[{"role": "user", "content": rewrite_prompt}],
                temperature=0.3,
                max_completion_tokens=200
            )

            result = response.choices[0].message.content.strip()
            # JSON ë°°ì—´ íŒŒì‹±
            if result.startswith("["):
                queries = json.loads(result)
                return queries[:3] if queries else [question]
            return [question]

        except Exception as e:
            print(f"   âš ï¸ Query rewrite failed, using original: {e}")
            return [question]

    # ==================== ê³µí†µ ë²¡í„° ê²€ìƒ‰ ë¡œì§ ====================

    def _vector_search(
        self,
        question: str,
        search_queries: List[str],
        top_k: int = 5,
    ) -> List[str]:
        """
        Azure AI Search í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ + ì‹œë§¨í‹± ë­í‚¹)

        Args:
            question: ì›ë³¸ ì§ˆë¬¸ (ì„ë² ë”©ìš©)
            search_queries: ê²€ìƒ‰ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (Query Rewrite ê²°ê³¼ í¬í•¨)
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜

        Returns:
            ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        """
        embedding_response = self.embedding_client.embeddings.create(
            input=[question],
            model=Config.EMBEDDING_DEPLOYMENT
        )
        query_vector = embedding_response.data[0].embedding
        vector_query = VectorizedQuery(vector=query_vector, k_nearest_neighbors=50, fields="text_vector")

        all_contexts = []
        seen_contexts: set = set()

        try:
            for search_query in search_queries:
                results = self.search_client.search(
                    search_text=search_query,
                    vector_queries=[vector_query],
                    select=["chunk", "parent_id"],
                    query_type="semantic",
                    semantic_configuration_name="my-semantic-config",
                    top=top_k
                )

                for r in results:
                    content = r.get('chunk') or r.get('content') or ""
                    source = r.get('parent_id') or "ì•Œ ìˆ˜ ì—†ëŠ” ì¶œì²˜"

                    context_entry = f"[ì¶œì²˜: {source}]\n{content}"
                    if content and context_entry not in seen_contexts:
                        seen_contexts.add(context_entry)
                        all_contexts.append(context_entry)

        except Exception as e:
            print(f"   âŒ Search failed: {e}")

        return all_contexts[:top_k * 2]

    def answer_question(
        self,
        question: str,
        model_key: Optional[str] = None,
        return_context: bool = False,
        top_k: int = 5,
        use_query_rewrite: bool = True
    ) -> Union[str, Tuple[str, List[str]]]:
        """
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ ì¦ê°• ìƒì„±(RAG)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

        1. Query Rewrite (ì„ íƒì ): ì§ˆë¬¸ì„ ì˜ë¯¸ì ìœ¼ë¡œ í™•ì¥
        2. AI Searchì—ì„œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰(ë²¡í„°+í‚¤ì›Œë“œ) ë° ì‹œë§¨í‹± ë­í‚¹ ìˆ˜í–‰
        3. ê²€ìƒ‰ëœ ë¬¸ë§¥(Context)ì„ ë°”íƒ•ìœ¼ë¡œ GPT-5.2ë¡œ ë‹µë³€ ìƒì„± (ì¶œì²˜ ì •ë³´ í¬í•¨)

        Args:
            question: ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë¬¸ìì—´.
            model_key: ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  íŠ¹ì • ëª¨ë¸ í‚¤.
            return_context: Trueì¼ ê²½ìš° ë‹µë³€ê³¼ í•¨ê»˜ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œì˜ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5).
            use_query_rewrite: Query Rewrite ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True).

        Returns:
            ë‹µë³€ ë¬¸ìì—´ ë˜ëŠ” (ë‹µë³€, ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸) íŠœí”Œ.
        """
        print(f"ğŸ” Searching for: {question} (top_k={top_k})")

        # 0. Query Rewrite (ì„ íƒì )
        search_queries = [question]
        if use_query_rewrite and self.enable_query_rewrite:
            search_queries = self._rewrite_query(question)
            if len(search_queries) > 1:
                print(f"   ğŸ“ Query expanded to {len(search_queries)} variants")

        # 1. ë²¡í„° ê²€ìƒ‰ (ê³µí†µ ë¡œì§)
        contexts = self._vector_search(question, search_queries, top_k)
        context_str = "\n\n".join(contexts)

        if not context_str:
            print("   âš ï¸ No relevant documentation found.")
            context_str = "ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        user_prompt = f"[Context]\n{context_str}\n\n[Question]\n{question}"

        # LLM í˜¸ì¶œì„ í†µí•œ ë‹µë³€ ìƒì„±
        answer = self.model_manager.get_completion(
            prompt=user_prompt,
            model_key=model_key,
            system_message=_RAG_SYSTEM_PROMPT
        )

        if return_context:
            return answer, contexts
        return answer

    # ==================== v4.0: Graph-Enhanced RAG ====================

    def graph_enhanced_answer(
        self,
        question: str,
        model_key: Optional[str] = None,
        return_context: bool = False,
        top_k: int = 5,
        use_query_rewrite: bool = True,
        graph_query_mode: str = "hybrid",
    ) -> Union[str, Tuple[str, List[str]]]:
        """
        [v4.0] Graph-Enhanced RAG: ë²¡í„° ê²€ìƒ‰ + Knowledge Graph ê²°í•©

        LightRAGì˜ Dual-Level Retrieval ê°œë…ì„ ì ìš©í•˜ì—¬:
        1. Azure AI Search í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ê¸°ì¡´ ë²¡í„°+í‚¤ì›Œë“œ)
        2. Knowledge Graph ë§¥ë½ ì •ë³´ (ì—”í‹°í‹°/ê´€ê³„)
        3. ë‘ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ë” í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€ ìƒì„±

        Args:
            question: ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë¬¸ìì—´.
            model_key: ë‹µë³€ ìƒì„±ì— ì‚¬ìš©í•  íŠ¹ì • ëª¨ë¸ í‚¤.
            return_context: Trueì¼ ê²½ìš° ë‹µë³€ê³¼ í•¨ê»˜ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
            top_k: ê²€ìƒ‰í•  ë¬¸ì„œì˜ ê°œìˆ˜ (ê¸°ë³¸ê°’: 5).
            use_query_rewrite: Query Rewrite ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True).
            graph_query_mode: Graph ê²€ìƒ‰ ëª¨ë“œ (local/global/hybrid/naive).

        Returns:
            ë‹µë³€ ë¬¸ìì—´ ë˜ëŠ” (ë‹µë³€, ì»¨í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸) íŠœí”Œ.
        """
        print(f"ğŸ” [Graph-Enhanced] Searching for: {question}")

        # === Part 1: ë²¡í„° ê²€ìƒ‰ (ê³µí†µ ë¡œì§) ===
        search_queries = [question]
        if use_query_rewrite and self.enable_query_rewrite:
            search_queries = self._rewrite_query(question)

        vector_contexts = self._vector_search(question, search_queries, top_k)

        # === Part 2: Knowledge Graph ê²€ìƒ‰ (v4.0 ì‹ ê·œ) ===
        graph_context = ""
        if self.graph_manager and graph_query_mode != "naive":
            try:
                from .graph_rag import QueryMode
                mode_map = {
                    "local": QueryMode.LOCAL,
                    "global": QueryMode.GLOBAL,
                    "hybrid": QueryMode.HYBRID,
                    "naive": QueryMode.NAIVE,
                }
                mode = mode_map.get(graph_query_mode, QueryMode.HYBRID)

                graph_result = self.graph_manager.query(
                    query_text=question,
                    mode=mode,
                    top_k=Config.GRAPH_TOP_K,
                )
                graph_context = graph_result.context_text
                if graph_context:
                    print(f"   ğŸ“Š Graph context: {len(graph_result.entities)} entities, "
                          f"{len(graph_result.relationships)} relationships")

            except Exception as e:
                print(f"   âš ï¸ Graph query failed: {e}")

        # === Part 3: ê²°í•©ëœ ì»¨í…ìŠ¤íŠ¸ë¡œ ë‹µë³€ ìƒì„± ===
        vector_context_str = "\n\n".join(vector_contexts)

        if not vector_context_str and not graph_context:
            print("   âš ï¸ No relevant documentation found.")
            vector_context_str = "ê´€ë ¨ëœ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # Graph ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        combined_context = vector_context_str
        if graph_context:
            combined_context = (
                f"[ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼]\n{vector_context_str}\n\n"
                f"[Knowledge Graph ë¶„ì„]\n{graph_context}"
            )

        user_prompt = f"[Context]\n{combined_context}\n\n[Question]\n{question}"

        answer = self.model_manager.get_completion(
            prompt=user_prompt,
            model_key=model_key,
            system_message=_GRAPH_RAG_SYSTEM_PROMPT
        )

        if return_context:
            return answer, vector_contexts
        return answer
