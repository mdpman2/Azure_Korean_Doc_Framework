"""
LightRAG-inspired Graph RAG ëª¨ë“ˆ (v4.0)

LightRAG(https://github.com/HKUDS/LightRAG)ì˜ í•µì‹¬ ê°œë…ì„ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„í•œ
ê²½ëŸ‰ Knowledge Graph ê¸°ë°˜ RAG ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

í•µì‹¬ ê¸°ëŠ¥:
- GPT-5.2ë¥¼ í™œìš©í•œ ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ (í•œêµ­ì–´ ìµœì í™”)
- NetworkX ê¸°ë°˜ ì¸ë©”ëª¨ë¦¬ Knowledge Graph
- Dual-Level Retrieval (Local: ì—”í‹°í‹° ì¤‘ì‹¬, Global: ê´€ê³„ ì¤‘ì‹¬)
- ë²¡í„° ê²€ìƒ‰ê³¼ ê·¸ë˜í”„ ê²€ìƒ‰ì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²°í•©

[2026-07 v4.0 ì‹ ê·œ]
- LightRAG ê¸°ë°˜ Graph RAG ì•„í‚¤í…ì²˜
- í•œêµ­ì–´ ì—”í‹°í‹° íƒ€ì… íŠ¹í™”
- Azure AI Search + Knowledge Graph í•˜ì´ë¸Œë¦¬ë“œ
"""

import json
import hashlib
from collections import deque
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

try:
    import networkx as nx
    HAS_NETWORKX = True
except ImportError:
    HAS_NETWORKX = False

from ..config import Config
from ..utils.azure_clients import AzureClientFactory


# ==================== ë°ì´í„° ëª¨ë¸ ====================

class QueryMode(Enum):
    """LightRAG ìŠ¤íƒ€ì¼ ê²€ìƒ‰ ëª¨ë“œ"""
    LOCAL = "local"       # ì—”í‹°í‹° ì¤‘ì‹¬ ê²€ìƒ‰ (Low-Level Keywords)
    GLOBAL = "global"     # ê´€ê³„/ì£¼ì œ ì¤‘ì‹¬ ê²€ìƒ‰ (High-Level Keywords)
    HYBRID = "hybrid"     # Local + Global ê²°í•©
    NAIVE = "naive"       # ë²¡í„° ê²€ìƒ‰ë§Œ ì‚¬ìš© (ê·¸ë˜í”„ ë¹„í™œìš©)


@dataclass
class Entity:
    """Knowledge Graph ì—”í‹°í‹°"""
    name: str
    entity_type: str
    description: str
    source_chunks: List[str] = field(default_factory=list)
    properties: Dict[str, Any] = field(default_factory=dict)

    @property
    def entity_id(self) -> str:
        return hashlib.md5(self.name.encode()).hexdigest()[:12]


@dataclass
class Relationship:
    """Knowledge Graph ê´€ê³„"""
    source: str          # ì†ŒìŠ¤ ì—”í‹°í‹° ì´ë¦„
    target: str          # íƒ€ê²Ÿ ì—”í‹°í‹° ì´ë¦„
    relation_type: str   # ê´€ê³„ ìœ í˜•
    description: str     # ê´€ê³„ ì„¤ëª…
    weight: float = 1.0
    keywords: str = ""
    source_chunks: List[str] = field(default_factory=list)


@dataclass
class GraphQueryResult:
    """ê·¸ë˜í”„ ê²€ìƒ‰ ê²°ê³¼"""
    entities: List[Entity] = field(default_factory=list)
    relationships: List[Relationship] = field(default_factory=list)
    context_text: str = ""
    source_chunks: List[str] = field(default_factory=list)


# ==================== í•œêµ­ì–´ ì—”í‹°í‹° íƒ€ì… ====================

KOREAN_ENTITY_TYPES = [
    "ì¸ë¬¼",          # Person
    "ì¡°ì§",          # Organization
    "ì¥ì†Œ",          # Location
    "ë‚ ì§œ",          # Date/Time
    "ë²•ë¥ ",          # Law/Regulation
    "ì •ì±…",          # Policy
    "ê¸°ê´€",          # Institution
    "ì‚¬ê±´",          # Event
    "ê¸°ìˆ ",          # Technology
    "ì œí’ˆ",          # Product
    "ê¸ˆì•¡",          # Amount/Money
    "ì§€í‘œ",          # Metric/Indicator
    "ë¬¸ì„œ",          # Document
    "ê°œë…",          # Concept
]


# ==================== ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ ====================

ENTITY_EXTRACTION_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œì—ì„œ ì—”í‹°í‹°(Entity)ì™€ ê´€ê³„(Relationship)ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì¶”ì¶œí•˜ì„¸ìš”:

### ì—”í‹°í‹° íƒ€ì…
{entity_types}

### ì¶œë ¥ í˜•ì‹ (JSON)
{{
  "entities": [
    {{
      "name": "ì—”í‹°í‹°ëª…",
      "entity_type": "ì—”í‹°í‹° íƒ€ì…",
      "description": "ì—”í‹°í‹°ì— ëŒ€í•œ ê°„ê²°í•œ ì„¤ëª… (1-2ë¬¸ì¥)"
    }}
  ],
  "relationships": [
    {{
      "source": "ì†ŒìŠ¤ ì—”í‹°í‹°ëª…",
      "target": "íƒ€ê²Ÿ ì—”í‹°í‹°ëª…",
      "relation_type": "ê´€ê³„ ìœ í˜•",
      "description": "ê´€ê³„ì— ëŒ€í•œ ì„¤ëª…",
      "keywords": "ê´€ë ¨ í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)",
      "weight": 1.0
    }}
  ]
}}

### ê·œì¹™
1. ì—”í‹°í‹°ëª…ì€ ì›ë¬¸ì— ë“±ì¥í•˜ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì¤„ì„ë§ë³´ë‹¤ ì •ì‹ ëª…ì¹­ ìš°ì„ )
2. ê´€ê³„ëŠ” ë‘ ì—”í‹°í‹° ê°„ì˜ ëª…í™•í•œ ì—°ê²°ì´ ìˆì„ ë•Œë§Œ ì¶”ì¶œ
3. weightëŠ” ê´€ê³„ì˜ ê°•ë„ (0.1 ~ 3.0, ê¸°ë³¸ 1.0)
4. í•œêµ­ì–´ ì—”í‹°í‹°ì— ì§‘ì¤‘í•˜ë˜, ì˜ë¬¸ ê³ ìœ ëª…ì‚¬ë„ í¬í•¨
5. ì¤‘ë³µ ì—”í‹°í‹°ëŠ” ë³‘í•©í•˜ì—¬ í•˜ë‚˜ë¡œ í‘œí˜„
"""

KEYWORD_EXTRACTION_PROMPT = """ì£¼ì–´ì§„ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ Knowledge Graph ê²€ìƒ‰ì— í•„ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

### ì¶œë ¥ í˜•ì‹ (JSON)
{{
  "high_level_keywords": ["ì£¼ì œ ìˆ˜ì¤€ í‚¤ì›Œë“œ (ê¸€ë¡œë²Œ ê´€ê³„/íŒ¨í„´ ê²€ìƒ‰ìš©)"],
  "low_level_keywords": ["êµ¬ì²´ì  ì—”í‹°í‹°/ì„¸ë¶€ì‚¬í•­ í‚¤ì›Œë“œ (ë¡œì»¬ ì—”í‹°í‹° ê²€ìƒ‰ìš©)"]
}}

### ì˜ˆì‹œ
ì§ˆë¬¸: "ì‚¼ì„±ì „ìì˜ 2025ë…„ ë°˜ë„ì²´ ë§¤ì¶œì€?"
â†’ high_level: ["ë°˜ë„ì²´ ì‚°ì—…", "ê¸°ì—… ë§¤ì¶œ ì‹¤ì "]
â†’ low_level: ["ì‚¼ì„±ì „ì", "2025ë…„", "ë°˜ë„ì²´ ë§¤ì¶œ"]

ì§ˆë¬¸: "{query}"
"""


class KnowledgeGraphManager:
    """
    LightRAG-inspired Knowledge Graph ê´€ë¦¬ì

    LightRAGì˜ í•µì‹¬ ì•„í‚¤í…ì²˜ë¥¼ ì°¸ì¡°í•˜ì—¬ êµ¬í˜„:
    - ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ (GPT-5.2)
    - NetworkX ê¸°ë°˜ ì¸ë©”ëª¨ë¦¬ ê·¸ë˜í”„
    - Dual-Level Retrieval (Local + Global)
    - í•œêµ­ì–´ ë¬¸ì„œ íŠ¹í™”

    ì°¸ì¡°: https://github.com/HKUDS/LightRAG
    """

    def __init__(
        self,
        entity_types: Optional[List[str]] = None,
        model_key: str = "gpt-5.2",
        max_entities_per_chunk: int = 20,
    ):
        if not HAS_NETWORKX:
            raise ImportError(
                "networkx íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì„¤ì¹˜: pip install networkx"
            )

        self.graph = nx.DiGraph()
        self.entity_types = entity_types or KOREAN_ENTITY_TYPES
        self.model_key = model_key
        self.max_entities_per_chunk = max_entities_per_chunk
        self._is_gpt5 = "gpt-5" in model_key.lower()

        # LLM í´ë¼ì´ì–¸íŠ¸
        self.client = AzureClientFactory.get_openai_client(is_advanced=True)
        self.model_name = Config.MODELS.get(model_key, "model-router")

        # ì—”í‹°í‹°/ê´€ê³„ ìºì‹œ (ì¤‘ë³µ ë°©ì§€)
        self._entity_cache: Dict[str, Entity] = {}
        self._chunk_to_entities: Dict[str, List[str]] = {}

        print(f"ğŸ“Š KnowledgeGraphManager ì´ˆê¸°í™” (ëª¨ë¸: {model_key}, ì—”í‹°í‹° íƒ€ì…: {len(self.entity_types)}ê°œ)")

    # ==================== ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ ====================

    def extract_from_chunks(
        self,
        chunks: List[Dict[str, Any]],
        batch_size: int = 5,
    ) -> Tuple[List[Entity], List[Relationship]]:
        """
        ë¬¸ì„œ ì²­í¬ì—ì„œ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì—¬ Knowledge Graphë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

        Args:
            chunks: ë¬¸ì„œ ì²­í¬ ë¦¬ìŠ¤íŠ¸ (page_content, metadata)
            batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ì²­í¬ ìˆ˜

        Returns:
            (ì¶”ì¶œëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸, ì¶”ì¶œëœ ê´€ê³„ ë¦¬ìŠ¤íŠ¸)
        """
        all_entities = []
        all_relationships = []

        entity_types_str = ", ".join(self.entity_types)
        system_prompt = ENTITY_EXTRACTION_SYSTEM_PROMPT.format(
            entity_types=entity_types_str
        )

        total = len(chunks)
        print(f"ğŸ” ì—”í‹°í‹°/ê´€ê³„ ì¶”ì¶œ ì‹œì‘ (ì´ {total}ê°œ ì²­í¬, ë°°ì¹˜ í¬ê¸°: {batch_size})")

        for i in range(0, total, batch_size):
            batch = chunks[i:i + batch_size]
            batch_text = "\n\n---\n\n".join([
                c.get("page_content", c) if isinstance(c, dict) else str(c)
                for c in batch
            ])

            # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ë‚´ê¸°
            if len(batch_text) > 8000:
                batch_text = batch_text[:8000]

            try:
                completion_params = {
                    "model": self.model_name,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:\n\n{batch_text}"}
                    ],
                    "temperature": 0.0,
                    "response_format": {"type": "json_object"},
                }

                if self._is_gpt5:
                    completion_params["max_completion_tokens"] = 4000
                else:
                    completion_params["max_tokens"] = 4000

                response = self.client.chat.completions.create(**completion_params)
                result_text = response.choices[0].message.content

                # JSON íŒŒì‹±
                result = json.loads(result_text)

                # ì—”í‹°í‹° ì²˜ë¦¬
                for ent_data in result.get("entities", []):
                    entity = Entity(
                        name=ent_data["name"],
                        entity_type=ent_data.get("entity_type", "ê°œë…"),
                        description=ent_data.get("description", ""),
                        source_chunks=[f"batch_{i // batch_size}"],
                    )
                    self._add_entity(entity)
                    all_entities.append(entity)

                # ê´€ê³„ ì²˜ë¦¬
                for rel_data in result.get("relationships", []):
                    relationship = Relationship(
                        source=rel_data["source"],
                        target=rel_data["target"],
                        relation_type=rel_data.get("relation_type", "ê´€ë ¨"),
                        description=rel_data.get("description", ""),
                        weight=float(rel_data.get("weight", 1.0)),
                        keywords=rel_data.get("keywords", ""),
                        source_chunks=[f"batch_{i // batch_size}"],
                    )
                    self._add_relationship(relationship)
                    all_relationships.append(relationship)

                print(f"   âœ… ë°°ì¹˜ {i // batch_size + 1}/{(total + batch_size - 1) // batch_size}: "
                      f"ì—”í‹°í‹° {len(result.get('entities', []))}ê°œ, "
                      f"ê´€ê³„ {len(result.get('relationships', []))}ê°œ ì¶”ì¶œ")

            except Exception as e:
                print(f"   âš ï¸ ë°°ì¹˜ {i // batch_size + 1} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                continue

        print(f"ğŸ“Š Knowledge Graph êµ¬ì¶• ì™„ë£Œ: "
              f"ë…¸ë“œ {self.graph.number_of_nodes()}ê°œ, "
              f"ì—£ì§€ {self.graph.number_of_edges()}ê°œ")

        return all_entities, all_relationships

    def _add_entity(self, entity: Entity) -> None:
        """ì—”í‹°í‹°ë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€ (ì¤‘ë³µ ì‹œ ë³‘í•©)"""
        name = entity.name

        if name in self._entity_cache:
            # ê¸°ì¡´ ì—”í‹°í‹°ì— ì •ë³´ ë³‘í•©
            existing = self._entity_cache[name]
            if entity.description and len(entity.description) > len(existing.description):
                existing.description = entity.description
            existing.source_chunks.extend(entity.source_chunks)
            # ê·¸ë˜í”„ ë…¸ë“œ ì—…ë°ì´íŠ¸
            self.graph.nodes[name]["description"] = existing.description
            self.graph.nodes[name]["source_count"] = len(existing.source_chunks)
        else:
            # ìƒˆ ì—”í‹°í‹° ì¶”ê°€
            self._entity_cache[name] = entity
            self.graph.add_node(
                name,
                entity_type=entity.entity_type,
                description=entity.description,
                source_count=len(entity.source_chunks),
            )

    def _add_relationship(self, rel: Relationship) -> None:
        """ê´€ê³„ë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€ (ì¤‘ë³µ ì‹œ ê°€ì¤‘ì¹˜ í•©ì‚°)"""
        if self.graph.has_edge(rel.source, rel.target):
            # ê¸°ì¡´ ê´€ê³„ì— ê°€ì¤‘ì¹˜ í•©ì‚°
            self.graph[rel.source][rel.target]["weight"] += rel.weight
            existing_desc = self.graph[rel.source][rel.target].get("description", "")
            if rel.description and rel.description not in existing_desc:
                self.graph[rel.source][rel.target]["description"] = f"{existing_desc}; {rel.description}"
        else:
            # ì†ŒìŠ¤/íƒ€ê²Ÿ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
            if not self.graph.has_node(rel.source):
                self.graph.add_node(rel.source, entity_type="ë¯¸ë¶„ë¥˜", description="", source_count=0)
            if not self.graph.has_node(rel.target):
                self.graph.add_node(rel.target, entity_type="ë¯¸ë¶„ë¥˜", description="", source_count=0)

            self.graph.add_edge(
                rel.source,
                rel.target,
                relation_type=rel.relation_type,
                description=rel.description,
                weight=rel.weight,
                keywords=rel.keywords,
            )

    # ==================== Dual-Level Retrieval ====================

    def query(
        self,
        query_text: str,
        mode: QueryMode = QueryMode.HYBRID,
        top_k: int = 10,
    ) -> GraphQueryResult:
        """
        LightRAG ìŠ¤íƒ€ì¼ Dual-Level Knowledge Graph ê²€ìƒ‰

        Args:
            query_text: ê²€ìƒ‰ ì§ˆì˜
            mode: ê²€ìƒ‰ ëª¨ë“œ (LOCAL/GLOBAL/HYBRID/NAIVE)
            top_k: ë°˜í™˜í•  ìµœëŒ€ ì—”í‹°í‹°/ê´€ê³„ ìˆ˜

        Returns:
            GraphQueryResult: ê²€ìƒ‰ëœ ì—”í‹°í‹°, ê´€ê³„, ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸
        """
        if mode == QueryMode.NAIVE:
            return GraphQueryResult()  # ê·¸ë˜í”„ ë¹„í™œìš©

        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        hl_keywords, ll_keywords = self._extract_keywords(query_text)
        print(f"   ğŸ”‘ Keywords - High: {hl_keywords}, Low: {ll_keywords}")

        result = GraphQueryResult()

        # 2. Local ê²€ìƒ‰ (ì—”í‹°í‹° ì¤‘ì‹¬)
        if mode in (QueryMode.LOCAL, QueryMode.HYBRID):
            local_entities, local_rels = self._local_search(ll_keywords, top_k)
            result.entities.extend(local_entities)
            result.relationships.extend(local_rels)

        # 3. Global ê²€ìƒ‰ (ê´€ê³„/ì£¼ì œ ì¤‘ì‹¬)
        if mode in (QueryMode.GLOBAL, QueryMode.HYBRID):
            global_entities, global_rels = self._global_search(hl_keywords, top_k)
            # ì¤‘ë³µ ì œê±° í›„ ì¶”ê°€
            existing_names = {e.name for e in result.entities}
            for e in global_entities:
                if e.name not in existing_names:
                    result.entities.append(e)
            existing_rels = {(r.source, r.target) for r in result.relationships}
            for r in global_rels:
                if (r.source, r.target) not in existing_rels:
                    result.relationships.append(r)

        # 4. ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
        result.context_text = self._build_context_text(result)

        print(f"   ğŸ“Š Graph Query ê²°ê³¼: ì—”í‹°í‹° {len(result.entities)}ê°œ, "
              f"ê´€ê³„ {len(result.relationships)}ê°œ")

        return result

    def _extract_keywords(self, query: str) -> Tuple[List[str], List[str]]:
        """LightRAG ìŠ¤íƒ€ì¼ Dual-Level í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            prompt = KEYWORD_EXTRACTION_PROMPT.format(query=query)

            completion_params = {
                "model": self.model_name,
                "messages": [
                    {"role": "system", "content": "í‚¤ì›Œë“œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.0,
                "response_format": {"type": "json_object"},
            }

            if self._is_gpt5:
                completion_params["max_completion_tokens"] = 500
            else:
                completion_params["max_tokens"] = 500

            response = self.client.chat.completions.create(**completion_params)
            result = json.loads(response.choices[0].message.content)

            hl = result.get("high_level_keywords", [])
            ll = result.get("low_level_keywords", [])
            return hl, ll

        except Exception as e:
            print(f"   âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # Fallback: ë‹¨ìˆœ ë¶„í• 
            words = query.split()
            return words[:2], words

    def _local_search(
        self,
        keywords: List[str],
        top_k: int,
    ) -> Tuple[List[Entity], List[Relationship]]:
        """Local Search: ì—”í‹°í‹° ì¤‘ì‹¬ (Low-Level Keywords ì‚¬ìš©)"""
        matched_entities = []
        seen_entity_names: set = set()
        rel_map: Dict[Tuple[str, str], Relationship] = {}

        for keyword in keywords:
            for node_name, node_data in self.graph.nodes(data=True):
                if keyword in node_name or keyword in node_data.get("description", ""):
                    if node_name not in seen_entity_names:
                        seen_entity_names.add(node_name)
                        matched_entities.append(Entity(
                            name=node_name,
                            entity_type=node_data.get("entity_type", ""),
                            description=node_data.get("description", ""),
                        ))

                    # ì—”í‹°í‹°ì˜ ì§ì ‘ ê´€ê³„ ìˆ˜ì§‘ (out + in edges)
                    for _, target, edge_data in self.graph.out_edges(node_name, data=True):
                        key = (node_name, target)
                        if key not in rel_map:
                            rel_map[key] = Relationship(
                                source=node_name, target=target,
                                relation_type=edge_data.get("relation_type", ""),
                                description=edge_data.get("description", ""),
                                weight=edge_data.get("weight", 1.0),
                                keywords=edge_data.get("keywords", ""),
                            )

                    for source, _, edge_data in self.graph.in_edges(node_name, data=True):
                        key = (source, node_name)
                        if key not in rel_map:
                            rel_map[key] = Relationship(
                                source=source, target=node_name,
                                relation_type=edge_data.get("relation_type", ""),
                                description=edge_data.get("description", ""),
                                weight=edge_data.get("weight", 1.0),
                                keywords=edge_data.get("keywords", ""),
                            )

        # weight ê¸°ì¤€ ì •ë ¬
        unique_rels = sorted(rel_map.values(), key=lambda r: r.weight, reverse=True)

        return matched_entities[:top_k], unique_rels[:top_k]

    def _global_search(
        self,
        keywords: List[str],
        top_k: int,
    ) -> Tuple[List[Entity], List[Relationship]]:
        """Global Search: ê´€ê³„/ì£¼ì œ ì¤‘ì‹¬ (High-Level Keywords ì‚¬ìš©)"""
        matched_relationships = []

        for keyword in keywords:
            # ê´€ê³„ì˜ description/keywordsì—ì„œ ë§¤ì¹­
            for source, target, edge_data in self.graph.edges(data=True):
                desc = edge_data.get("description", "")
                kw = edge_data.get("keywords", "")
                rel_type = edge_data.get("relation_type", "")

                if keyword in desc or keyword in kw or keyword in rel_type:
                    rel = Relationship(
                        source=source,
                        target=target,
                        relation_type=rel_type,
                        description=desc,
                        weight=edge_data.get("weight", 1.0),
                        keywords=kw,
                    )
                    matched_relationships.append(rel)

        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        seen = set()
        unique_rels = []
        for r in matched_relationships:
            key = (r.source, r.target)
            if key not in seen:
                seen.add(key)
                unique_rels.append(r)
        unique_rels.sort(key=lambda r: r.weight, reverse=True)

        # ê´€ê³„ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
        entity_names = set()
        for r in unique_rels[:top_k]:
            entity_names.add(r.source)
            entity_names.add(r.target)

        entities = []
        for name in entity_names:
            if self.graph.has_node(name):
                node_data = self.graph.nodes[name]
                entities.append(Entity(
                    name=name,
                    entity_type=node_data.get("entity_type", ""),
                    description=node_data.get("description", ""),
                ))

        return entities[:top_k], unique_rels[:top_k]

    def _build_context_text(self, result: GraphQueryResult) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        parts = []

        if result.entities:
            parts.append("### Knowledge Graph ì—”í‹°í‹°")
            for e in result.entities:
                parts.append(f"- **{e.name}** ({e.entity_type}): {e.description}")

        if result.relationships:
            parts.append("\n### Knowledge Graph ê´€ê³„")
            for r in result.relationships:
                parts.append(
                    f"- {r.source} --[{r.relation_type}]--> {r.target}: {r.description}"
                )

        return "\n".join(parts) if parts else ""

    # ==================== ê·¸ë˜í”„ ìœ í‹¸ë¦¬í‹° ====================

    def get_stats(self) -> Dict[str, Any]:
        """Knowledge Graph í†µê³„ ë°˜í™˜"""
        if not self.graph.nodes:
            return {"nodes": 0, "edges": 0, "entity_types": {}}

        entity_type_counts = {}
        for _, data in self.graph.nodes(data=True):
            et = data.get("entity_type", "ë¯¸ë¶„ë¥˜")
            entity_type_counts[et] = entity_type_counts.get(et, 0) + 1

        return {
            "nodes": self.graph.number_of_nodes(),
            "edges": self.graph.number_of_edges(),
            "entity_types": entity_type_counts,
            "avg_degree": (
                sum(d for _, d in self.graph.degree()) / self.graph.number_of_nodes()
                if self.graph.number_of_nodes() > 0 else 0
            ),
        }

    def get_subgraph(
        self,
        entity_name: str,
        max_depth: int = 2,
        max_nodes: int = 50,
    ) -> Dict[str, Any]:
        """íŠ¹ì • ì—”í‹°í‹° ì¤‘ì‹¬ì˜ ì„œë¸Œê·¸ë˜í”„ ë°˜í™˜ (LightRAG get_knowledge_graph ì°¸ì¡°)"""
        if not self.graph.has_node(entity_name):
            return {"nodes": [], "edges": []}

        # BFSë¡œ ì„œë¸Œê·¸ë˜í”„ íƒìƒ‰ (dequeë¡œ O(1) popleft)
        visited = {entity_name}
        queue = deque([(entity_name, 0)])
        edges = []

        while queue and len(visited) < max_nodes:
            current, depth = queue.popleft()
            if depth >= max_depth:
                continue

            # Out-edges
            for _, neighbor, data in self.graph.out_edges(current, data=True):
                edges.append({
                    "source": current,
                    "target": neighbor,
                    "relation_type": data.get("relation_type", ""),
                    "weight": data.get("weight", 1.0),
                })
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor, depth + 1))

            # In-edges
            for neighbor, _, data in self.graph.in_edges(current, data=True):
                edges.append({
                    "source": neighbor,
                    "target": current,
                    "relation_type": data.get("relation_type", ""),
                    "weight": data.get("weight", 1.0),
                })
                if neighbor not in visited:
                    visited.add(neighbor)
                    queue.append((neighbor, depth + 1))

        nodes = []
        for name in visited:
            if self.graph.has_node(name):
                node_data = self.graph.nodes[name]
                nodes.append({
                    "name": name,
                    "entity_type": node_data.get("entity_type", ""),
                    "description": node_data.get("description", ""),
                    "degree": self.graph.degree(name),
                })

        return {"nodes": nodes, "edges": edges}

    def save_graph(self, filepath: str) -> None:
        """Knowledge Graphë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        data = {
            "nodes": [
                {
                    "name": n,
                    **self.graph.nodes[n],
                }
                for n in self.graph.nodes
            ],
            "edges": [
                {
                    "source": u,
                    "target": v,
                    **d,
                }
                for u, v, d in self.graph.edges(data=True)
            ],
        }
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ Knowledge Graph ì €ì¥ ì™„ë£Œ: {filepath}")

    def load_graph(self, filepath: str) -> None:
        """JSON íŒŒì¼ì—ì„œ Knowledge Graph ë¡œë“œ"""
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)

        self.graph.clear()
        for node in data.get("nodes", []):
            name = node.pop("name")
            self.graph.add_node(name, **node)

        for edge in data.get("edges", []):
            source = edge.pop("source")
            target = edge.pop("target")
            self.graph.add_edge(source, target, **edge)

        print(f"ğŸ“ Knowledge Graph ë¡œë“œ ì™„ë£Œ: ë…¸ë“œ {self.graph.number_of_nodes()}ê°œ, "
              f"ì—£ì§€ {self.graph.number_of_edges()}ê°œ")

    def clear(self) -> None:
        """Knowledge Graph ì´ˆê¸°í™”"""
        self.graph.clear()
        self._entity_cache.clear()
        self._chunk_to_entities.clear()
        print("ğŸ—‘ï¸ Knowledge Graph ì´ˆê¸°í™” ì™„ë£Œ")
