# Azure Korean Document Understanding & Retrieval Framework

ì´ í”„ë ˆì„ì›Œí¬ëŠ” í•œêµ­ì–´ ë¬¸ì„œì˜ ê¹Šì€ ì´í•´(Deep Document Understanding)ì™€ íš¨ìœ¨ì ì¸ ê²€ìƒ‰(Retrieval)ì„ ìœ„í•´ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. Tencentì˜ WeKnora ë° Microsoft Agent Frameworkì˜ ì£¼ìš” íŒ¨í„´ì„ ì°¨ìš©í•˜ì—¬ Azure AI ê¸°ìˆ ê³¼ í•œêµ­ì–´ ìµœì í™” ë¡œì§ì„ ê²°í•©í–ˆìŠµë‹ˆë‹¤.

## ğŸš€ ì£¼ìš” íŠ¹ì§•

- **Multi-Model Support**: GPT-4.1, GPT-5.2, claude-sonnet-4-5, claude-opus-4-5ë“± ë‹¤ì–‘í•œ LLMì„ ë™ì ìœ¼ë¡œ êµì²´í•˜ë©° ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
- **Hybrid Document Parsing**: Azure Document Intelligenceì™€ GPT-4.1ì„ ì—°ë™í•˜ì—¬ í…ìŠ¤íŠ¸ë¿ë§Œ ì•„ë‹ˆë¼ ì°¨íŠ¸, í‘œ, ì´ë¯¸ì§€ì˜ ì˜ë¯¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.
- **Korean Semantic Chunking**: ë‹¨ìˆœ ê¸¸ì´ ê¸°ë°˜ ë¶„í• ì´ ì•„ë‹Œ, ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ì™€ ë¬¸ë§¥ì˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ëŠ” ì‹œë§¨í‹± ì²­í‚¹ì„ ì§€ì›í•©ë‹ˆë‹¤.
- **Azure AI Search Integration**: í•œêµ­ì–´ ìµœì í™” ë¶„ì„ê¸°(`ko.microsoft`)ì™€ ë²¡í„° ê²€ìƒ‰ì„ í™œìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```text
azure_korean_doc_framework/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agent.py               # RAG ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë° ë‹µë³€ ìƒì„± ê¸°ë°˜
â”‚   â”œâ”€â”€ multi_model_manager.py # GPT/Claude ë©€í‹° ëª¨ë¸ ê´€ë¦¬ ë° í˜¸ì¶œ
â”‚   â””â”€â”€ vector_store.py        # Azure AI Search ì¸ë±ìŠ¤ ë° ì—…ë¡œë“œ ê´€ë¦¬
â”œâ”€â”€ parsing/
â”‚   â”œâ”€â”€ parser.py              # Azure DI + GPT Vision ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì„œ
â”‚   â””â”€â”€ chunker.py             # ë§ˆí¬ë‹¤ìš´ í—¤ë” ë° ì‹œë§¨í‹± ì²­ì»¤
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ azure_clients.py       # Azure ì„œë¹„ìŠ¤ í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬
â”œâ”€â”€ config.py                  # í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ê´€ë¦¬
â””â”€â”€ README.md                  # ë³¸ ë¬¸ì„œ
```

## ğŸ› ï¸ ì„¤ì¹˜ ë° ì„¤ì •

### 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
```bash
pip install openai azure-ai-documentintelligence azure-search-documents langchain langchain-openai langchain-experimental pdf2image pillow python-dotenv
```
> [!NOTE]
> PDF ì‹œê°ì  ë¶„ì„ ê¸°ëŠ¥ì„ ìœ„í•´ì„œëŠ” ì‹œìŠ¤í…œì— `poppler`ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
í”„ë¡œì íŠ¸ ë£¨íŠ¸ í´ë”ì— `.env` íŒŒì¼ì„ ìƒì„±í•˜ê³  `.env.template`ì˜ ë‚´ìš©ì„ ë³µì‚¬í•˜ì—¬ Azure ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.

```env
AZURE_OPENAI_API_KEY=...
AZURE_OPENAI_ENDPOINT=...
MODEL_DEPLOYMENT_GPT4_1=gpt-4.1
AZURE_SEARCH_ENDPOINT=...
# ... (ê¸°íƒ€ ì„¤ì •)
```

## ğŸ“– ì‚¬ìš© ë°©ë²•

### ê¸°ë³¸ ì‹¤í–‰ (`main.py`)
í”„ë ˆì„ì›Œí¬ì˜ ì „ì²´ íë¦„(ë¬¸ì„œ íŒŒì‹± -> ì²­í‚¹ -> ê²€ìƒ‰ ì¸ë±ì‹± -> ë©€í‹° ëª¨ë¸ ì‹¤ìŠµ)ì„ í•œ ë²ˆì— í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
python main.py
```

### ì½”ë“œ ì˜ˆì‹œ

#### 1. ì¸ì½”ë”© ë° ë©€í‹° ëª¨ë¸ ë‹µë³€
```python
from azure_korean_doc_framework.core.agent import KoreanDocAgent

agent = KoreanDocAgent()
# GPT-5.1 ë˜ëŠ” Claude ë“± ì›í•˜ëŠ” ëª¨ë¸ ëª…ì‹œ ê°€ëŠ¥
answer = agent.answer_question("íšŒì‚¬ì˜ ë³µì§€ ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì¤˜", model_key="gpt-5.2")
print(answer)
```

#### 2. í•˜ì´ë¸Œë¦¬ë“œ ë¬¸ì„œ íŒŒì‹±
```python
from azure_korean_doc_framework.parsing.parser import HybridDocumentParser

parser = HybridDocumentParser()
markdown_text = parser.parse("document.pdf")
print(markdown_text) # í…ìŠ¤íŠ¸ + í‘œ + ì´ë¯¸ì§€ ì„¤ëª…ì´ í¬í•¨ëœ ë§ˆí¬ë‹¤ìš´
```

#### 3. ì „ì²´ ì‚¬ìš© ë°©ë²• : íŒŒì‹± > ì²­í‚¹ > ì¸ë±ì‹± > Q&A
```python
import os
from azure_korean_doc_framework.parsing.parser import HybridDocumentParser
from azure_korean_doc_framework.parsing.chunker import KoreanSemanticChunker
from azure_korean_doc_framework.core.vector_store import VectorStore
from azure_korean_doc_framework.core.agent import KoreanDocAgent

def main():
    print("ğŸŒŸ Welcome to Azure Korean Document Understanding & Retrieval Framework ğŸŒŸ")

    # 1. Initialize Components
    parser = HybridDocumentParser()
    chunker = KoreanSemanticChunker()
    vector_store = VectorStore()

    # 2. Document Ingestion (Example)
    # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”
    sample_file = "RAG_TEST_DATA/(1) 2024 ë‹¬ë¼ì§€ëŠ” ì„¸ê¸ˆì œë„.pdf"

    if os.path.exists(sample_file):
        print(f"\n--- [Phase 1: Ingestion - {sample_file}] ---")
        # [ìˆ˜ì •] íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸ ë° ì—…ë°ì´íŠ¸ ë¡œì§ ì ìš©
        file_mod_time = os.path.getmtime(sample_file)

        vector_store.create_index_if_not_exists()

        # ìµœì‹  ìƒíƒœì¸ì§€ í™•ì¸
        if vector_store.is_file_up_to_date(os.path.basename(sample_file), file_mod_time):
             print(f"â© File is up-to-date. Skipping parsing/upload.")
        else:
            print(f"ğŸ”„ File updated or new. Processing...")
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì—…ë°ì´íŠ¸ ì‹œ)
            vector_store.delete_documents_by_parent_id(os.path.basename(sample_file))

            # íŒŒì‹± ë° ì²­í‚¹
            markdown_content = parser.parse(sample_file)

            # ë©”íƒ€ë°ì´í„°ì— íŒŒì¼ëª…ê³¼ ìˆ˜ì • ì‹œê°„ ì¶”ê°€
            extra_meta = {
                "source": os.path.basename(sample_file),
                "last_modified": file_mod_time
            }
            chunks = chunker.chunk(markdown_content, extra_metadata=extra_meta)

            vector_store.upload_documents(chunks)
    else:
        print(f"\nâ„¹ï¸ Skip ingestion: {sample_file} not found. Running Q&A with existing search index.")

    # 3. Multi-Model Q&A Demonstration
    agent = KoreanDocAgent()
    question = "ì´ ë¬¸ì„œì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í•µì‹¬ ìš”ì•½ ì„¸ ê°€ì§€ë§Œ ë§í•´ì¤˜."

    models_to_test = ["gpt-4.1", "gpt-5.2", "claude-sonnet-4-5"]

    print("\n--- [Phase 2: Multi-Model Q&A] ---")
    print(f"User Question: {question}")

    for model in models_to_test:
        print(f"\n--- Model: {model} ---")
        answer = agent.answer_question(question, model_key=model)
        print(f"Response:\n{answer}")

if __name__ == "__main__":
    main()
```

## ğŸ¤ ì°¸ì¡° í”„ë¡œì íŠ¸
- [Tencent WeKnora](https://github.com/Tencent/WeKnora)
- [Microsoft Agent Framework Samples](https://github.com/microsoft/agent-framework)

